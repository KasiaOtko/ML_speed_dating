---
title: "Regression"
author: "Katarzyna Otko"
date: "3 11 2020"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Linear regression model predicting positive rate

```{r}
library(tidyverse)
library(tidymodels)

setwd("C:/Users/katin/Desktop/Folder/STUDIA/DTU/Semestr I/Intro to ML/ML_speed_dating")

SD <- read.csv('SD_clean.csv')

# percentage of getting dec_o = 1 (if a participant had 10 dates and 5 partners chose dec = 1, then positive rate = 50%)
  group_by(iid) %>%
  summarise(Rate = sum(dec_o)/n())

match_rate <- subset(SD, select = c('iid', 'match', 'dec', 'dec_o')) %>%
  group_by(iid) %>%
  summarise(match_rate = mean(match)) 

means_no_NA <- SD %>% subset(select = c('iid', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'fun_o', 'like_o')) %>%
  group_by(iid) %>%
  summarise(across(attr_o:like_o, mean)) %>%
  drop_na()

dataset <- left_join(means_no_NA, positive_rate, by = 'iid')


```


# According to scripts
Regression, part A - one-layer CV

```{r}
library(glmnet)
library(cvTools)
dataset <- left_join(means_no_NA, positive_rate, by = 'iid')
dataset <- dataset[, 2:9] # We dont want iid column
dataset <- cbind(const = rep(1,nrow(dataset)), dataset) # adding constant

K = 10
CV <- cvFolds(nrow(dataset), K=K)


M <- ncol(X)
lambda_tmp <- c(0, 10^seq(-3, 8, length.out = 20))
T <- length(lambda_tmp)

w <- matrix(nrow = M,ncol = K)
rownames(w) <- colnames(X)
coeff_per_lambda <- list()

Error_train <- matrix(nrow = T, ncol = K)
Error_test <- matrix(nrow = T, ncol = K)

for (i in 1:length(lambda_tmp)) {
  
    for (k in 1:K) {
    X_train <- X[CV$subsets[CV$which!=k], ];
    y_train <- y[CV$subsets[CV$which!=k]];
    X_test <- X[CV$subsets[CV$which==k], ];
    y_test <- y[CV$subsets[CV$which==k]];
    CV$TrainSize[k] <- length(y_train)
    CV$TestSize[k] <- length(y_test)
    
    mu <- colMeans(X_train[, 2:8])
    sigma <- apply(X_train[, 2:8], 2, sd)
    
    X_train[, 2:8] <- scale(X_train[, 2:8], mu, sigma)
    X_test[, 2:8] <- scale(X_test[, 2:8], mu, sigma)
      
    CV$TrainSize[k] <- length(y_train)
    CV$TestSize[k] <- length(y_test)
      
    Xty <- t(X_train) %*% y_train
    XtX <- t(X_train) %*% as.matrix(X_train)
    
    lambdaI = lambda_tmp[i]*diag(M);
    lambdaI[1, 1] = 0
    w[, k] <- solve(XtX+lambdaI) %*% Xty
    
    Error_train[i,k] = sum((y_train - as.matrix(X_train) %*% w[,k])^2)
    Error_test[i,k] = sum((y_test - as.matrix(X_test) %*% w[,k])^2)
    }
  coeff_per_lambda[[i]] <- w
}

train_error <- rowMeans(Error_train/CV$TrainSize)
test_error <- rowMeans(Error_test/CV$TestSize)

lambda_opt <- which.min(test_error)
lambda_tmp[lambda_opt]

# coefficients for optimal lambda

sort(rowMeans(coeff_per_lambda[[lambda_opt]]), decreasing = TRUE)
sort(rowMeans(coeff_per_lambda[[1]]), decreasing = TRUE)

```

## PLOTS

```{r}
par(mfrow=c(1,2))
par(mgp=c(2.5,1,0))

# Plot coefficients
w_mean <- matrix(nrow = M, ncol = T)
rownames(w_mean) <- rownames(w)
for (i in 1:length(coeff_per_lambda)) {
  lambda <- coeff_per_lambda[[i]]
  w_mean[,i] <- rowMeans(lambda)
}
w_mean <- w_mean[order(w_mean[,1], decreasing = T),]

colors_vector=rainbow(8)
plot(log(lambda_tmp), w_mean[2,], type = 'l', col = colors_vector[2], ylim = c(min(w_mean),0.13),
     ylab = 'Mean value of coefficients', xlab = 'Log(lambda)', main = 'Weights as a function of lambda')
points(log(lambda_tmp), w_mean[2,], col = colors_vector[2])

for (j in 3:M) {
  lines(log(lambda_tmp), w_mean[j,])
}

for(i in 3:M){
  points(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
  lines(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
}
abline(v = log(lambda_tmp[lambda_opt]), lty = 2)
legend_names <- names(w_mean[,1])[2:8]
legend("topright", legend = c(legend_names, 'Opt. lambda'), lty = c(rep(1, 7), 2), col = c(colors_vector[2:8], 'black'), cex = .9)

# Train/test error
plot(log(lambda_tmp), train_error, type = 'l',
      xlab="Log(lambda)", ylab="Error", main = 'Generalization error as a function of lambda',
     ylim = c(min(test_error), max(train_error)), col = 'red')
lines(log(lambda_tmp), test_error, col = 'blue')
legend('bottomright', legend = c('Test error', 'Training error', 'Opt. lambda'), col = c('blue', 'red', 'black'), lty= c(1, 1, 2), cex = .95)
abline(v = log(lambda_tmp[lambda_opt]), lty = 2)
text(13.2, 0.0255, paste0('Optimal lambda: ', round(lambda_tmp[lambda_opt], 2)))

coeff <- as.data.frame(sort(rowMeans(coeff_per_lambda[[lambda_opt]])[2:8], decreasing = TRUE))
#coeff$name <- names(coeff)
colnames(coeff) <- 'Coefficient'

coeff %>% ggplot(aes(Coefficient, reorder(rownames(coeff), Coefficient))) + geom_point(size = 4) +
  geom_vline(xintercept = 0, lty = 2) + 
  xlab('Value') + ylab('Coefficient') + ggtitle('Coefficients\' values for the optimal lambda')+
  theme_minimal() + # Changing theme
  theme(plot.title = element_text(hjust = 0.5), # Centers title
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12), # x/y labels position
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size = 12)) 

```

# Regression, part B - two layer CV

```{r}

X <- dataset[, 1:8]
y <- dataset[, 9]
M <- ncol(X)
# Outer fold
K <- 10
#set.seed(1234) # for reproducibility
CV <- cvFolds(nrow(dataset), K=K)
# set up vectors that will store sizes of training and test sizes
CV$TrainSize <- c()
CV$TestSize <- c()

# values of lambda
lambda_tmp <- c(0, 10^seq(-3, 8, length.out = 20))
T <- length(lambda_tmp)
# Initialise variables
KK <- 10 # inner loop
temp <- rep(NA, M*T*KK); 
w <- array(temp, c(M, T, KK));  

Error_train2 <- matrix(rep(NA, times=T*KK), nrow = T)
Error_test2 <- matrix(rep(NA, times=T*KK), nrow = T)
lambda_opt <- rep(NA, K)
w_rlr <- matrix(rep(NA, times=M*K), nrow=M)
Error_train_rlr <- rep(NA,K)
Error_test_rlr <- rep(NA,K)
w_noreg <- matrix(rep(NA, times=M*K), nrow=M)
mu <- matrix(rep(NA, times=(M-1)*K), nrow=K)
sigma <- matrix(rep(NA, times=(M-1)*K), nrow=K)
Error_train <- rep(NA,K)
Error_test <- rep(NA,K)
Error_train_nofeatures <- rep(NA,K)
Error_test_nofeatures <- rep(NA,K)


for(k in 1:K){

  print(paste('Crossvalidation fold ', k, '/', K, sep=''))
  
  # Extract the training and test set
  X_train <- X[CV$subsets[CV$which!=k], ];
  y_train <- y[CV$subsets[CV$which!=k]];
  X_test <- X[CV$subsets[CV$which==k], ];
  y_test <- y[CV$subsets[CV$which==k]];
  CV$TrainSize[k] <- length(y_train)
  CV$TestSize[k] <- length(y_test)
  
  # Use 10-fold crossvalidation to estimate optimal value of lambda    
  KK <- 10
  
  CV2 <- cvFolds( dim(X_train)[1], K=KK)
  CV2$TrainSize <- c()
  CV2$TestSize <- c()
  
  
  for(kk in 1:KK){
  
    X_train2 <- X_train[CV2$subsets[CV2$which!=kk], ]
    y_train2 <- y_train[CV2$subsets[CV2$which!=kk]]
    X_test2 <- X_train[CV2$subsets[CV2$which==kk], ]
    y_test2 <- y_train[CV2$subsets[CV2$which==kk]]
    
    mu2 <- colMeans(X_train2[, 2:8])
    sigma2 <- apply(X_train2[, 2:8], 2, sd)
    
    X_train2[, 2:8] <- scale(X_train2[, 2:8], mu2, sigma2)
    X_test2[, 2:8] <- scale(X_test2[, 2:8], mu2, sigma2)
    
    CV2$TrainSize[kk] <- length(y_train)
    CV2$TestSize[kk] <- length(y_test2)
    
    Xty2 <- t(X_train2) %*% y_train2
    XtX2 <- t(X_train2) %*% as.matrix(X_train2)
    
    for(t in 1:length(lambda_tmp)){
      
      # Learn parameter for current value of lambda for the given inner CV_fold
      lambdaI = lambda_tmp[t]*diag(M);
      lambdaI[1,1] = 0; # don't regularize bias
      w[,t,kk] <- solve(XtX2+lambdaI) %*% Xty2
      
      # Evaluate training and test performance 
      Error_train2[t,kk] = sum((y_train2 - as.matrix(X_train2) %*% w[,t,kk])^2)
      Error_test2[t,kk] = sum((y_test2 - as.matrix(X_test2) %*% w[,t,kk])^2)
      
    }
  }
  
  # Display result for cross-validation fold
  w_mean <- apply(w, c(1,2), mean)
  
  # Plot weights as a function of the regularization strength (not offset)
  par(mfrow=c(1,2))
  plot(log(lambda_tmp), w_mean[2,], xlab="log(lambda)",
       ylab="Coefficient Values",main=paste("Weights, fold ",k,"/",K),
       ylim = c(min(w_mean[-1,]), max(w_mean[-1,])))
  lines(log(lambda_tmp), w_mean[2,])
  
  colors_vector = colors()[c(1,50,26,59,101,126,151,551,71,257,506,634,639,383)]

  for(i in 3:M){
    points(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
    lines(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
  }
  
  #matplot(log(lambda_tmp), t(w_mean), type = c("b"),pch=1,col = 1:4) #plot
  
  # Select optimal value of lambda
  ind_opt <- which.min(apply(Error_test2, 1, sum) / sum(CV2$TestSize))
  lambda_opt[k] <- lambda_tmp[ind_opt]
  
  
  par(cex.main=2) # Define size of title
  par(cex.lab=2) # Define size of axis labels
  par(cex.axis=2) # Define size of axis labels
  par(mar=c(5,6,4,1)+.1) # Increase margin size to allow for larger axis labels
  plot(log(lambda_tmp), log(apply(Error_train2,1,sum)/sum(CV2$TrainSize)), 
       xlab="log(lambda)", ylab="log(Error)" , 
       main = paste("Otimal lambda: 1e",log10(lambda_opt[k])))
  
  lines(log(lambda_tmp), log(apply(Error_train2,1,sum)/sum(CV2$TrainSize)))
  
  points(log(lambda_tmp), log(apply(Error_test2,1,sum)/sum(CV2$TestSize)) ,col="red")   
  lines(log(lambda_tmp), log(apply(Error_test2,1,sum)/sum(CV2$TestSize)) , col="red")
  
  legend("bottomright", legend=c("Training","Test"), col=c("black","red"), lty=1)
  
  # Standardize outer fold based on training set, and save the mean and standard
  # deviations since they're part of the model (they would be needed for
  # making new predictions) - for brevity we won't always store these in the scripts
  mu[k,] <- colMeans(X_train[, 2:8])
  sigma[k,] <- apply(X_train[, 2:8], 2, sd)
  
  X_train[, 2:8] <- scale(X_train[, 2:8], mu[k,], sigma[k,])
  X_test[, 2:8] <- scale(X_test[, 2:8], mu[k,], sigma[k,])
  
  # Estimate w for the optimal value of lambda
  Xty = t(X_train) %*% y_train
  XtX = t(X_train) %*% as.matrix(X_train)
  
  lambdaI = lambda_opt[k] * diag(M)
  lambdaI[1,1] = 0; # don't regularize bias
  
  w_rlr[,k] = solve(XtX+lambdaI) %*% Xty
       
  # evaluate training and test error performance for optimal selected value of lambda
  Error_train_rlr[k] = sum( (y_train - as.matrix(X_train) %*% w_rlr[,k])^2 )
  Error_test_rlr[k] = sum( (y_test - as.matrix(X_test) %*% w_rlr[,k])^2 )
  
  # Compute squared error without regularization
  w_noreg[,k] = solve(XtX) %*% Xty
  Error_train[k] = mean( (y_train - as.matrix(X_train) %*% w_noreg[,k])^2);
  Error_test[k] = mean( (y_test - as.matrix(X_test) %*% w_noreg[,k])^2);
  
  # Compute squared error without using the input data at all
  Error_train_nofeatures[k] = mean((y_train - mean(y_train))^2);
  Error_test_nofeatures[k] = mean((y_test - mean(y_train))^2);
  
}

Gen_error <- mean(Error_test); Gen_error
Error_test
round(lambda_tmp, 2)
lambda_opt

# Per every inner fold for every lambda:
Error_test2/CV2$TestSize

mean(Error_test_nofeatures)

```












```{r}
train_idx <- seq(1, 0.8*nrow(dataset))
test_idx <- seq(430, nrow(dataset))
X_train <- dataset[train_idx, 1:8]
X_test <- dataset[test_idx, 1:8]

y_train <- dataset[train_idx, 9]
y_test <- dataset[test_idx, 9]
dim(X_train)

x <- as.matrix(X_train)
y <- as.matrix(y_train)

l1_net<- cv.glmnet(x, y, alpha=0)

best_lambda_lasso <- l1_net$lambda.1se  # largest lambda in 1 SE
lasso_coef <- l1_net$glmnet.fit$beta[,  # retrieve coefficients
              l1_net$glmnet.fit$lambda  # at lambda.1se
              == best_lambda_lasso]


lasso_coef

l1_net$lambda.min

lambda_tmp <- 10^seq(-3, 5, length.out = 20)
#lambda_tmp <- c(0, 0.04, 0.045, 0.05, 0.05451746, 0.06, 0.065, 0.07, 1)

ridge_cv <- cv.glmnet(x, y, alpha = 0,  
                      lambda = lambda_tmp, 
                      standardize = TRUE, nfolds = 10) 
  
# Plot cross-validation results 
plot(ridge_cv) 

lambda_cv <- ridge_cv$lambda.min 
lambda_cv
# Fit final model, get its sum of squared 
# residuals and multiple R-squared 
model_cv <- glmnet(x, y, alpha = 0, lambda = lambda_cv, 
                   standardize = TRUE) 
y_hat_cv <- predict(model_cv, as.matrix(X_test)) 
ssr_cv <- t(y_test - y_hat_cv) %*% (y_test - y_hat_cv) 
rsq_ridge_cv <- cor(y_test, y_hat_cv)^2 

res <- cv.glmnet(x, y, alpha = 0, lambda = lambda_tmp, 
              standardize = TRUE) 
plot(res, xvar = "lambda") 
legend("bottomright", lwd = 1, col = 1:6,  
       legend = colnames(X), cex = .7)



```



# First, basic linear regression model

```{r}


X <- as.matrix(dataset) #dataset[, c(2, 3, 5, 7)]
# Feature transformation
X <- t(apply(X, 1, '-', colMeans(X))) # subtracting mean
X <- t(apply(X, 1, '/', apply(X, 2, sd))) # dividing by standard deviation
apply(X, 2, mean)
apply(X, 2, sd)

Y <- as.matrix(dataset[, 'Rate'])

cor(X) # colinearity between Attr and Like :/
cor(dataset[, c(2, 3, 5, 7, 8)])
model <- lm(Y ~ X)
summary(model)

y_est <- model$fitted.values
residuals <- model$residuals

plot(Y, y_est)

# Residuals are normally distributed!
hist(Y-y_est)
shapiro.test(residuals)$p.value

# Mean squared error
mean((Y-y_est)^2)
mean((Y-y_est)^2)/mean(Y)
```

```{r}
library('tidymodels')

dataset <- left_join(means_no_NA, positive_rate, by = 'iid')

set.seed(123)
dataset_split <-initial_split(dataset[, c(2:9)], strata = 'Rate') #equal number of classes for each split
train <- training(dataset_split)
test <- testing(dataset_split)

# validation sets for tuning the model
set.seed(345)
folds <- vfold_cv(train)
folds

data_rec <- recipe(Rate ~ ., data = train) %>%
  step_normalize(all_predictors()) %>%
  prep(training = train)

wf <- workflow() %>%
  add_recipe(data_rec)

# Linear regression
tune_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_engine('glmnet') #%>%
  #fit(Rate ~ ., data = juice(data_rec))

lambda_grid <- grid_regular(penalty(), levels = 30)

#doParallel::registerDoParallel()

set.seed(2020)
ridge_grid <- tune_grid(
  wf %>% add_model(tune_spec),
  resamples = folds,
  grid = lambda_grid
)

collect_metrics(ridge_grid)

ridge_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```


```{r}
library('tidymodels')

dataset <- left_join(means_no_NA, positive_rate, by = 'iid')

set.seed(123)
dataset_split <-initial_split(dataset[, c(2:8)], strata = 'Rate') #equal number of classes for each split
train <- training(dataset_split)
test <- testing(dataset_split)

# validation sets for tuning the model
set.seed(345)
folds <- vfold_cv(train)
folds

data_rec <- recipe(Rate ~ ., data = train) 

# Linear regression
lm_spec <- linear_reg(penalty = 0.001, mixture = 0) %>%
  set_engine('glmnet')

# Random forsets
rf_spec <- rand_forest(trees = 1000) %>%
  set_engine('ranger') %>%
  set_mode('regression') 

#install.packages('doParallel')
#library(doParallel)
doParallel::registerDoParallel()

# Workflow - fitting on resamples
lm_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metric_set(rmse, mae, rsq_trad),
    control = control_resamples(save_pred = T) # saves predictions
  )

#library('ranger')
rf_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metric_set(rmse, mae, rsq_trad),
    control = control_resamples(save_pred = T) # saves predictions
  )

# every recipe is being executed on every fold and then random forest model is being fit on it and evaluated on the hold out set in 'splits' (resample)

collect_metrics(lm_wf)
collect_metrics(rf_wf)

# fitting on training data
final <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec) %>%
  last_fit(dataset_split)

collect_metrics(final)
collect_predictions(final)

library(tidyverse)
# plotting estimates without an intercept
final %>% pull(.workflow) %>% # similar to final$.workflow
  pluck(1) %>%
  tidy() %>%
  filter(term != '(Intercept)') %>%
  ggplot(aes(estimate, fct_reorder(term, estimate))) + geom_point(size = 4) +
  geom_vline(xintercept = 0, lty = 2) +
  geom_errorbar(aes(xmin = estimate - std.error, 
                    xmax = estimate + std.error),
                width = 0.3) + theme_minimal()
```


