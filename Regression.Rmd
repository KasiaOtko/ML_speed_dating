---
title: "Regression"
author: "Katarzyna Otko"
date: "3 11 2020"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Linear regression model predicting positive rate

```{r}
library(tidyverse)
library(tidymodels)

setwd("C:/Users/katin/Desktop/Folder/STUDIA/DTU/Semestr I/Intro to ML/ML_speed_dating")

SD <- read.csv('SD_clean.csv')

# percentage of getting dec_o = 1 (if a participant had 10 dates and 5 partners chose dec = 1, then positive rate = 50%)
positive_rate <- SD %>% subset(select = c('iid', 'dec', 'dec_o')) %>%
  group_by(iid) %>%
  summarise(Rate = sum(dec_o)/n())

match_rate <- subset(SD, select = c('iid', 'match', 'dec', 'dec_o')) %>%
  group_by(iid) %>%
  summarise(match_rate = mean(match)) 

means_no_NA <- SD %>% subset(select = c('iid', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'fun_o', 'like_o')) %>%
  group_by(iid) %>%
  summarise(across(attr_o:like_o, mean)) %>%
  drop_na()

dataset <- left_join(means_no_NA, positive_rate, by = 'iid')


```


# According to scripts
Regression, part A - one-layer CV

```{r}
library(glmnet)
library(cvTools)
dataset <- left_join(means_no_NA, positive_rate, by = 'iid')
dataset <- dataset[, 2:9] # We dont want iid column
dataset <- cbind(const = rep(1,nrow(dataset)), dataset) # adding constant

K = 10
CV <- cvFolds(nrow(dataset), K=K)


M <- ncol(X)
lambda_tmp <- c(0, 10^seq(-3, 8, length.out = 20))
T <- length(lambda_tmp)

w <- matrix(nrow = M,ncol = K)
rownames(w) <- colnames(X)
coeff_per_lambda <- list()

Error_train <- matrix(nrow = T, ncol = K)
Error_test <- matrix(nrow = T, ncol = K)

for (i in 1:length(lambda_tmp)) {
  
    for (k in 1:K) {
    X_train <- X[CV$subsets[CV$which!=k], ];
    y_train <- y[CV$subsets[CV$which!=k]];
    X_test <- X[CV$subsets[CV$which==k], ];
    y_test <- y[CV$subsets[CV$which==k]];
    CV$TrainSize[k] <- length(y_train)
    CV$TestSize[k] <- length(y_test)
    
    mu <- colMeans(X_train[, 2:8])
    sigma <- apply(X_train[, 2:8], 2, sd)
    
    X_train[, 2:8] <- scale(X_train[, 2:8], mu, sigma)
    X_test[, 2:8] <- scale(X_test[, 2:8], mu, sigma)
      
    CV$TrainSize[k] <- length(y_train)
    CV$TestSize[k] <- length(y_test)
      
    Xty <- t(X_train) %*% y_train
    XtX <- t(X_train) %*% as.matrix(X_train)
    
    lambdaI = lambda_tmp[i]*diag(M);
    lambdaI[1, 1] = 0
    w[, k] <- solve(XtX+lambdaI) %*% Xty
    
    Error_train[i,k] = sum((y_train - as.matrix(X_train) %*% w[,k])^2)
    Error_test[i,k] = sum((y_test - as.matrix(X_test) %*% w[,k])^2)
    }
  coeff_per_lambda[[i]] <- w
}

train_error <- rowMeans(Error_train/CV$TrainSize)
test_error <- rowMeans(Error_test/CV$TestSize)

lambda_opt <- which.min(test_error)
lambda_tmp[lambda_opt]

# coefficients for optimal lambda

sort(rowMeans(coeff_per_lambda[[lambda_opt]]), decreasing = TRUE)
sort(rowMeans(coeff_per_lambda[[1]]), decreasing = TRUE)

```

## PLOTS

```{r}
par(mfrow=c(1,2))
par(mgp=c(2.5,1,0))

# Plot coefficients
w_mean <- matrix(nrow = M, ncol = T)
rownames(w_mean) <- rownames(w)
for (i in 1:length(coeff_per_lambda)) {
  lambda <- coeff_per_lambda[[i]]
  w_mean[,i] <- rowMeans(lambda)
}
w_mean <- w_mean[order(w_mean[,1], decreasing = T),]

colors_vector=rainbow(8)
plot(log(lambda_tmp), w_mean[2,], type = 'l', col = colors_vector[2], ylim = c(min(w_mean),0.13),
     ylab = 'Mean value of coefficients', xlab = 'Log(lambda)', main = 'Weights as a function of lambda')
points(log(lambda_tmp), w_mean[2,], col = colors_vector[2])

for (j in 3:M) {
  lines(log(lambda_tmp), w_mean[j,])
}

for(i in 3:M){
  points(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
  lines(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
}
abline(v = log(lambda_tmp[lambda_opt]), lty = 2)
legend_names <- names(w_mean[,1])[2:8]
legend("topright", legend = c(legend_names, 'Opt. lambda'), lty = c(rep(1, 7), 2), col = c(colors_vector[2:8], 'black'), cex = .9)

# Train/test error
plot(log(lambda_tmp), train_error, type = 'l',
      xlab="Log(lambda)", ylab="Error", main = 'Generalization error as a function of lambda',
     ylim = c(min(test_error), max(train_error)), col = 'red')
lines(log(lambda_tmp), test_error, col = 'blue')
legend('bottomright', legend = c('Test error', 'Training error', 'Opt. lambda'), col = c('blue', 'red', 'black'), lty= c(1, 1, 2), cex = .95)
abline(v = log(lambda_tmp[lambda_opt]), lty = 2)
text(13.2, 0.0255, paste0('Optimal lambda: ', round(lambda_tmp[lambda_opt], 2)))

coeff <- as.data.frame(sort(rowMeans(coeff_per_lambda[[lambda_opt]])[2:8], decreasing = TRUE))
#coeff$name <- names(coeff)
colnames(coeff) <- 'Coefficient'

coeff %>% ggplot(aes(Coefficient, reorder(rownames(coeff), Coefficient))) + geom_point(size = 4) +
  geom_vline(xintercept = 0, lty = 2) + 
  xlab('Value') + ylab('Coefficient') + ggtitle('Coefficients\' values for the optimal lambda')+
  theme_minimal() + # Changing theme
  theme(plot.title = element_text(hjust = 0.5), # Centers title
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12), # x/y labels position
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size = 12)) 

```

# Alvils's part

```{r}

###############
# data init
#############
data = dataset[,2:ncol(dataset)]
x = data[, -ncol(data)]
y = data[, ncol(data)]
x = scale(x)
transformed_data = as.data.frame(cbind(x, rate=y))


#################
#L2 reg
################

l2_reg <- function(train_source, train_target, test_source, test_target, lambda) {

    # train model
    net <- glmnet(data.matrix(train_source), 
                        data.matrix(train_target), 
                        alpha = 0,                             
                        lambda = lambda)
    
    # train error
    train_res = predict(net, data.matrix(train_source) ,s=lambda)
    train_error = mean((train_res - train_target)^2)
    
    # test error
    test_res = predict(net, data.matrix(test_source) ,s=lambda)
    test_error = mean((test_res - test_target)^2)

    return (list(train_error, test_error))
}



outer_nb_folds = 10
inner_nb_folds = 10
params = seq(0, 1, by = 0.1)
nb_params = length(params)
best_model_hyperparam = rep(0, (nb_params - 1))
error_per_fold = rep(0, outer_nb_folds)

set.seed(123)
# creates nested crossvalidation both inside and outside fold is stratified
folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(v = outer_nb_folds), 
                     inside = vfold_cv(v = inner_nb_folds))


# outer fold 
for (i in 1:outer_nb_folds) {
  # divides in outer train/test
  outer_data = folds$splits[[i]]$data
  idx_outer_train = folds$splits[[i]]$in_id
  idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
  outer_train = outer_data[idx_outer_train, ]
  outer_test = outer_data[idx_outer_test, ]
  inner_error = matrix(0L, nrow = inner_nb_folds, ncol = nb_params)
  
  # inner fold
  for (j in 1:inner_nb_folds) {
    # divides in inner train/test
    inner_data = folds$inner_resamples[[i]]$splits[[j]]$data
    idx_inner_train = folds$inner_resamples[[i]]$splits[[j]]$in_id
    idx_inner_test = -which(as.numeric(rownames(inner_data)) %in% idx_inner_train)
    inner_train = inner_data[idx_inner_train, ]
    inner_test = inner_data[idx_inner_test, ]
    
    # hyperparam optim
    for (k in 1:nb_params) {
      # trains model and gets train and test error
      error = l2_reg(inner_train[, -ncol(inner_train)],
                     inner_train[, ncol(inner_train)],
                     inner_test[, -ncol(inner_test)],
                     inner_test[, ncol(inner_test)],
                     params[k])
      # inner error for each inner fold
      inner_error[j, k] = error[[2]]
    }
  }
  
  # calculates avg error for each model in inner fold and trains once more where the generalization error is minimum
  min_idx = which.min(colMeans(inner_error))
  best_model_hyperparam[i] = min_idx
  error = l2_reg(outer_train[, -ncol(outer_train)],
                 outer_train[, ncol(outer_train)],
                 outer_test[, -ncol(outer_test)],
                 outer_test[, ncol(outer_test)],
                 params[min_idx])
  error_per_fold[i] = error[[2]]

}

# for table for project 2
l2_gen_error = mean(error_per_fold); l2_gen_error
l2_error_per_fold = error_per_fold; l2_error_per_fold
l2_params = params; l2_params
l2_best_hyperparam_per_fold = best_model_hyperparam; l2_best_hyperparam_per_fold
```

```{r}
#################
#Base model
################

base <- function(train_source, train_target, test_source, test_target) {
    # train error
    train_res = mean(train_target)
    train_error = mean((train_res - train_target)^2)
    # test error
    test_res = train_res
    test_error = mean((test_res - test_target)^2)
    return (list(train_error, test_error))
}



outer_nb_folds = 10
inner_nb_folds = 10
params = c(1)
nb_params = length(params)
best_model_hyperparam = rep(0, (nb_params - 1))
error_per_fold = rep(0, outer_nb_folds)

set.seed(123)
# creates nested crossvalidation both inside and outside fold is stratified
folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(v = outer_nb_folds), 
                     inside = vfold_cv(v = inner_nb_folds))


# outer fold 
for (i in 1:outer_nb_folds) {
  # divides in outer train/test
  outer_data = folds$splits[[i]]$data
  idx_outer_train = folds$splits[[i]]$in_id
  idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
  outer_train = outer_data[idx_outer_train, ]
  outer_test = outer_data[idx_outer_test, ]
  inner_error = matrix(0L, nrow = inner_nb_folds, ncol = nb_params)
  
  # inner fold
  for (j in 1:inner_nb_folds) {
    # divides in inner train/test
    inner_data = folds$inner_resamples[[i]]$splits[[j]]$data
    idx_inner_train = folds$inner_resamples[[i]]$splits[[j]]$in_id
    idx_inner_test = -which(as.numeric(rownames(inner_data)) %in% idx_inner_train)
    inner_train = inner_data[idx_inner_train, ]
    inner_test = inner_data[idx_inner_test, ]
    # hyperparam optim
    for (k in 1:nb_params) {
      # trains model and gets train and test error
      error = base(inner_train[, -ncol(inner_train)],
                     inner_train[, ncol(inner_train)],
                     inner_test[, -ncol(inner_test)],
                     inner_test[, ncol(inner_test)])
      # inner error for each inner fold
      inner_error[j, k] = error[[2]]
    }
  }
  
  # calculates avg error for each model in inner fold and trains once more where the generalization error is minimum
  min_idx = which.min(colMeans(inner_error))
  best_model_hyperparam[i] = min_idx
  error = base(outer_train[, -ncol(outer_train)],
                 outer_train[, ncol(outer_train)],
                 outer_test[, -ncol(outer_test)],
                 outer_test[, ncol(outer_test)])
  error_per_fold[i] = error[[2]]

}

# for table for project 2
base_gen_error = mean(error_per_fold)
base_error_per_fold = error_per_fold
base_params = params
base_best_hyperparam_per_fold = best_model_hyperparam
```


# Regression, part B - two layer CV, Kasia's part

```{r}

X <- dataset[, 1:8]
y <- dataset[, 9]
M <- ncol(X)
# Outer fold
K <- 10
#set.seed(1234) # for reproducibility
CV <- cvFolds(nrow(dataset), K=K)
# set up vectors that will store sizes of training and test sizes
CV$TrainSize <- c()
CV$TestSize <- c()

# values of lambda
lambda_tmp <- c(0, 10^seq(-3, 8, length.out = 20))
T <- length(lambda_tmp)
# Initialise variables
KK <- 10 # inner loop
temp <- rep(NA, M*T*KK); 
w <- array(temp, c(M, T, KK));  

Error_train2 <- matrix(rep(NA, times=T*KK), nrow = T)
Error_test2 <- matrix(rep(NA, times=T*KK), nrow = T)
lambda_opt <- rep(NA, K)
w_rlr <- matrix(rep(NA, times=M*K), nrow=M)
Error_train_rlr <- rep(NA,K)
Error_test_rlr <- rep(NA,K)
w_noreg <- matrix(rep(NA, times=M*K), nrow=M)
mu <- matrix(rep(NA, times=(M-1)*K), nrow=K)
sigma <- matrix(rep(NA, times=(M-1)*K), nrow=K)
Error_train <- rep(NA,K)
Error_test <- rep(NA,K)
Error_train_nofeatures <- rep(NA,K)
Error_test_nofeatures <- rep(NA,K)


for(k in 1:K){

  print(paste('Crossvalidation fold ', k, '/', K, sep=''))
  
  # Extract the training and test set
  X_train <- X[CV$subsets[CV$which!=k], ];
  y_train <- y[CV$subsets[CV$which!=k]];
  X_test <- X[CV$subsets[CV$which==k], ];
  y_test <- y[CV$subsets[CV$which==k]];
  CV$TrainSize[k] <- length(y_train)
  CV$TestSize[k] <- length(y_test)
  
  # Use 10-fold crossvalidation to estimate optimal value of lambda    
  KK <- 10
  
  CV2 <- cvFolds( dim(X_train)[1], K=KK)
  CV2$TrainSize <- c()
  CV2$TestSize <- c()
  
  
  for(kk in 1:KK){
  
    X_train2 <- X_train[CV2$subsets[CV2$which!=kk], ]
    y_train2 <- y_train[CV2$subsets[CV2$which!=kk]]
    X_test2 <- X_train[CV2$subsets[CV2$which==kk], ]
    y_test2 <- y_train[CV2$subsets[CV2$which==kk]]
    
    mu2 <- colMeans(X_train2[, 2:8])
    sigma2 <- apply(X_train2[, 2:8], 2, sd)
    
    X_train2[, 2:8] <- scale(X_train2[, 2:8], mu2, sigma2)
    X_test2[, 2:8] <- scale(X_test2[, 2:8], mu2, sigma2)
    
    CV2$TrainSize[kk] <- length(y_train)
    CV2$TestSize[kk] <- length(y_test2)
    
    Xty2 <- t(X_train2) %*% y_train2
    XtX2 <- t(X_train2) %*% as.matrix(X_train2)
    
    for(t in 1:length(lambda_tmp)){
      
      # Learn parameter for current value of lambda for the given inner CV_fold
      lambdaI = lambda_tmp[t]*diag(M);
      lambdaI[1,1] = 0; # don't regularize bias
      w[,t,kk] <- solve(XtX2+lambdaI) %*% Xty2
      
      # Evaluate training and test performance 
      Error_train2[t,kk] = sum((y_train2 - as.matrix(X_train2) %*% w[,t,kk])^2)
      Error_test2[t,kk] = sum((y_test2 - as.matrix(X_test2) %*% w[,t,kk])^2)
      
    }
  }
  
  # Display result for cross-validation fold
  w_mean <- apply(w, c(1,2), mean)
  
  # Plot weights as a function of the regularization strength (not offset)
  par(mfrow=c(1,2))
  plot(log(lambda_tmp), w_mean[2,], xlab="log(lambda)",
       ylab="Coefficient Values",main=paste("Weights, fold ",k,"/",K),
       ylim = c(min(w_mean[-1,]), max(w_mean[-1,])))
  lines(log(lambda_tmp), w_mean[2,])
  
  colors_vector = colors()[c(1,50,26,59,101,126,151,551,71,257,506,634,639,383)]

  for(i in 3:M){
    points(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
    lines(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
  }
  
  #matplot(log(lambda_tmp), t(w_mean), type = c("b"),pch=1,col = 1:4) #plot
  
  # Select optimal value of lambda
  ind_opt <- which.min(apply(Error_test2, 1, sum) / sum(CV2$TestSize))
  lambda_opt[k] <- lambda_tmp[ind_opt]
  
  
  par(cex.main=2) # Define size of title
  par(cex.lab=2) # Define size of axis labels
  par(cex.axis=2) # Define size of axis labels
  par(mar=c(5,6,4,1)+.1) # Increase margin size to allow for larger axis labels
  plot(log(lambda_tmp), log(apply(Error_train2,1,sum)/sum(CV2$TrainSize)), 
       xlab="log(lambda)", ylab="log(Error)" , 
       main = paste("Otimal lambda: 1e",log10(lambda_opt[k])))
  
  lines(log(lambda_tmp), log(apply(Error_train2,1,sum)/sum(CV2$TrainSize)))
  
  points(log(lambda_tmp), log(apply(Error_test2,1,sum)/sum(CV2$TestSize)) ,col="red")   
  lines(log(lambda_tmp), log(apply(Error_test2,1,sum)/sum(CV2$TestSize)) , col="red")
  
  legend("bottomright", legend=c("Training","Test"), col=c("black","red"), lty=1)
  
  # Standardize outer fold based on training set, and save the mean and standard
  # deviations since they're part of the model (they would be needed for
  # making new predictions) - for brevity we won't always store these in the scripts
  mu[k,] <- colMeans(X_train[, 2:8])
  sigma[k,] <- apply(X_train[, 2:8], 2, sd)
  
  X_train[, 2:8] <- scale(X_train[, 2:8], mu[k,], sigma[k,])
  X_test[, 2:8] <- scale(X_test[, 2:8], mu[k,], sigma[k,])
  
  # Estimate w for the optimal value of lambda
  Xty = t(X_train) %*% y_train
  XtX = t(X_train) %*% as.matrix(X_train)
  
  lambdaI = lambda_opt[k] * diag(M)
  lambdaI[1,1] = 0; # don't regularize bias
  
  w_rlr[,k] = solve(XtX+lambdaI) %*% Xty
       
  # evaluate training and test error performance for optimal selected value of lambda
  Error_train_rlr[k] = sum( (y_train - as.matrix(X_train) %*% w_rlr[,k])^2 )
  Error_test_rlr[k] = sum( (y_test - as.matrix(X_test) %*% w_rlr[,k])^2 )
  
  # Compute squared error without regularization
  w_noreg[,k] = solve(XtX) %*% Xty
  Error_train[k] = mean( (y_train - as.matrix(X_train) %*% w_noreg[,k])^2);
  Error_test[k] = mean( (y_test - as.matrix(X_test) %*% w_noreg[,k])^2);
  
  # Compute squared error without using the input data at all
  Error_train_nofeatures[k] = mean((y_train - mean(y_train))^2);
  Error_test_nofeatures[k] = mean((y_test - mean(y_train))^2);
  
}

Gen_error <- mean(Error_test); Gen_error
Error_test
round(lambda_tmp, 2)
lambda_opt

# Per every inner fold for every lambda:
Error_test2/CV2$TestSize

mean(Error_test_nofeatures)

```
