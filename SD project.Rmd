---
output: html_document
editor_options: 
  chunk_output_type: console
---
## Authors

Katarzyna Otko - s202872  
Alvils Sture - s202586

```{r, message=F, warning=F}
#library(rapport)
#library(rapportools)
library(devtools)
library(dplyr)
library(ggplot2)
library(reshape2)
library(tidyr)
library(rpart)
setwd('C:/Users/katin/Desktop/Folder/STUDIA/DTU/Semestr I/Intro to ML/Project I')

#setwd("/home/nomow/Documents/DTU/Intro_to_machine_learning/ML_speed_dating")

# mode functions
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

SD <- read.csv('Speed Dating Data.csv')

dim(SD)

# number of women
length(unique(SD$iid[which(SD$gender == 0)])) # 274

# number of men
length(unique(SD$iid[which(SD$gender == 1)])) # 277

```


## Data cleaning
```{r}
NAs <- sapply(SD, function(x) sum(is.na(x)))
sort(NAs[which(NAs > 0)]) 

#filling one missing value in last id row
SD[which(is.na(SD$id)), 1:2] <- 22

# filling 10 missing values in pid columns
SD[which(is.na(SD$pid)), 1:15] # partner's id - 7
SD[which(SD$id == 7 & SD$wave == 5), 1:2] # we have to fill these 10 NAs with 128
SD[which(is.na(SD$pid)), 'pid'] <- 128

# filling missing values in columns <attribute_name>1_1
DF <- SD[, c('attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1')]
df <- DF[!complete.cases(DF),]
dim(df); sum(rowSums(df, na.rm = T) == 100) # so there are 42 rows where we can impute 0s
df[which(df[,1] + df[,2] + df[,3] == 100),]
df[which(df[,1] + df[,2] + df[,3] == 100), c(4:6)] <- 0
df[which(df[,1] + df[,2] + df[,3] + df[,4] == 100),]
df[which(df[,1] + df[,2] + df[,3] + df[,4] == 100), c(5:6)] <- 0
df[which(df[,1] + df[,2] + df[,3] + df[,4] +df[, 5] == 100),]
df[which(df[,1] + df[,2] + df[,3] + df[,4] +df[, 5] == 100), 6] <- 0
DF[!complete.cases(DF),] <- df
SD[,70:75] <- DF

## data filtering
# assigns Mode value for each iid group where value is NA
cols = c("imprelig", "imprace", "goal", "date", "go_out",  "sports", "tvsports", "exercise", "dining", "museums", "art", "hiking","gaming", "clubbing", "reading", "tv", "theater", "movies", "concerts", "music", "shopping", "yoga", "exphappy") 
for (col in cols) {
    pf_o <- SD %>%group_by(iid) %>%  summarize_at( .vars = col, .funs = Mode)
    idx = which(is.na(SD[, col]) == T)
    pf_o[is.na(pf_o)] = 0
    SD[idx, col] = pf_o[SD[idx,]$iid, 2]
}
# assigns Mean value for each iid group where value is NA
cols = c("attr1_1",  "sinc1_1",  "intel1_1", "fun1_1",   "amb1_1" ,  "shar1_1",  "attr2_1",  "sinc2_1", "intel2_1",  "fun2_1",   "amb2_1" ,  "shar2_1", "attr1_2",  "sinc1_2", "intel1_2", "fun1_2",   "amb1_2",   "shar1_2",  "attr3_2",  "sinc3_2",  "intel3_2", "fun3_2" ,"amb3_2", "attr3_1", "sinc3_1", "fun3_1", "intel3_1" ,"amb3_1", "attr", "sinc", "intel", "fun", "amb", "shar", "like", "prob")
for (col in cols) {
    pf_o <- SD %>%group_by(iid) %>%  summarize_at( .vars = col, .funs = mean)
    idx = which(is.na(SD[, col]) == T)
    pf_o[is.na(pf_o)] = 0
    SD[idx, col] = pf_o[SD[idx,]$iid, 2]
}

## age_o filtering
age_o <- SD %>%group_by(iid) %>%  summarise(mode = Mode(age))
idx = which(is.na(SD$age_o) == T)
SD[idx, "age_o"] = age_o[SD[idx,]$pid, ]$mode

## race_o filtering
race_o <- SD %>%group_by(iid) %>%  summarise(mode = Mode(race))
idx = which(is.na(SD$race_o) == T)
SD[idx, "race_o"] = age_o[SD[idx,]$pid, ]$mode

## attr_o and pf_o filtering
cols = c("pf_o_att", "pf_o_sin", "pf_o_int", "pf_o_fun", "pf_o_amb", "pf_o_sha", "dec_o", "attr_o", "sinc_o", "intel_o", "fun_o", "amb_o", "shar_o", "like_o",  "prob_o")
for (col in cols) {
    pf_o <- SD %>%group_by(pid) %>%  summarize_at( .vars = col, .funs = mean)
    idx = which(is.na(SD[, col]) == T)
    pf_o[is.na(pf_o)] = 0
    SD[idx, col] = pf_o[SD[idx,]$pid, 2]
}

# adding one column with explanation for race column (matching index with race names)
race_idx <- unique(SD$race)
race_val <- c('Asian', 'European', 'Other', 'Latino', 'Black', NA)
SD$race_explained <- race_val[match(SD$race, race_idx)]
SD$race_explained_o <- race_val[match(SD$race_o, race_idx)]

# adding one column with explanation for field_cd column (matching index with race names)
field_idx <- c(1:18, NA)
field_val <- c('Law', 'Math', 'Social Science, Psychologist', 'Medical Science/Pharmaceuticals/Bio Tech',
               'Engineering', 'English/Creative Writing/ Journalism', 'History/Religion/Philosophy',
              'Business/Econ/Finance', 'Education, Academia', 'Biological Sciences/Chemistry/Physics', 
              'Social Work', 'Undergrad/undecided', 'Political Science/International Affairs',
              'Film', 'Fine Arts/Arts Administration', 'Languages', 'Architecture', 'Other', 'Other')
SD$field_explained <- field_val[match(SD$field_cd, field_idx)]

# converting income from string to numeric
SD$income <- as.numeric(gsub(',', "", SD$income, fixed = T))

## removes any cols where the nb of NA values are greater than 0.2 (20%)
# SD[SD==""]<-NA
# SD = SD[, colSums(is.na(SD)) / dim(SD)[1] < 0.2]

# adds additional columns based on pid
cols = c("imprelig", "imprace", "goal", "date", "go_out",  "sports", "tvsports", "exercise", "dining", "museums", "art", "hiking","gaming", "clubbing", "reading", "tv", "theater", "movies", "concerts", "music", "shopping", "yoga", "exphappy") 
for (col in cols) {
    SD[,  paste(col, "_o", sep="")] = 0 
}
for(row in 1:nrow(SD)) {
  pid = SD[row, "pid"]
  tmp = SD[which(SD$iid == pid)[1], ]
  for (col in cols) {
      SD[row, paste(col, "_o", sep="")] = tmp[col] 
  }
}

## Normalizing values for waves 6 - 9
# Waves 6 - 9:
# attr4_1 - shar4_1 have values between 0 and 10
# attr2_1 - shar2_1 OK
# attr1_2 - shar1_2 OK
summary(SD[SD$wave >= 6 & SD$wave <= 9, c('attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1')]) # each feature is between 1-10, while they all should add up to 100
df <- SD[SD$wave >= 6 & SD$wave <= 9, c('attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1')]
df_norm <- df
for(col in 1:ncol(df)) {
  df_norm[[colnames(df)[col]]] <- df[[colnames(df)[col]]]/rowSums(df, na.rm = T)*100
}
unique(rowSums(df_norm, na.rm = T))
SD[SD$wave >= 6 & SD$wave <= 9,c('attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1')] <- df_norm
head(SD[SD$wave >= 6 & SD$wave <= 9,c('attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1')])
  
# to remove
#cols = c("iid", "id", "idg", "condtn", "wave", "round", "position",  "order", "partner", "pid", "field", "field_cd",  "race", "from", "zipcode", "career", "career_c", "dec", "match_es", "satis_2", "length", "numdat_2", 'race', 'race_o', 'field_cd', 'field', 'int_corr', "dec_o", "race_explained","race_explained_o", "field_explained", "met", "met_o")

#SD = SD[ , -which(names(SD) %in% cols)]
#SD = SD[complete.cases(SD), ]

```

## Outliers
```{r}
df <- subset(SD, !duplicated(SD$iid), select = c('attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'age', 'income')) %>% scale() %>% as.data.frame()

df_long <- pivot_longer(df[complete.cases(df),], colnames(df))
attr_idx <- unique(df_long$name)
df_long$name <- factor(df_long$name , levels=attr_idx)

df_long %>% ggplot(aes(x = name, y = value, fill = name)) + geom_boxplot(show.legend = F) + 
  xlab('Feature') + ylab('Value') + ggtitle('Distribution of chosen features after scaling') +
  theme_minimal() + # Changing theme
  theme(plot.title = element_text(hjust = 0.5, size = 15), # Centers title
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 15), # x/y labels position
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size = 15),
        axis.text.x = element_text(size = 13),
        axis.text.y = element_text(size = 13)) 

df_long %>% ggplot(aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~ name, scales = 'free') +
  theme_minimal() + # Changing theme
  theme(plot.title = element_text(hjust = 0.5), # Centers title
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12), # x/y labels position
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size = 12)) 
```

## Normality
```{r}
df <- subset(SD, !duplicated(SD$iid), select = c('attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'age', 'income')) %>% scale() %>% as.data.frame()

library('moments')
kurtosis(df, na.rm = T)
skewness(df, na.rm = T)

df_long <- pivot_longer(df[complete.cases(df),], colnames(df))
attr_idx <- unique(df_long$name)
df_long$name <- factor(df_long$name , levels=attr_idx)
par(mfrow=c(2, 4))
for (i in 1:ncol(df)) {
  qqnorm(df[,i])
  qqline(df[,i])
}

df_long %>% ggplot(aes(value)) + geom_histogram(bins = 20) +
  facet_wrap(~name, ncol = 4, scales = 'free')

for (i in 1:ncol(df)) {
  h <- hist(df[,i], main = paste("Histogram of", colnames(df)[i]), xlab = "Value", ylab = "")
  mtext(text = "Frequency", side = 2, line = 2.2, cex = 0.7)
  ec <- ecdf(df[,i])
  lines(x = h$mids, y = ec(h$mids)*max(h$counts), col ='red')
  axis(4, at=seq(from = 0, to = max(h$counts), length.out = 11), labels=seq(0, 1, 0.1), col = 'red', col.axis = 'red')
}

library(tseries)
pvalues <- c()
for(i in 1:ncol(df)) {
  pvalues[i] <- shapiro.test(df[complete.cases(df),i])$p.value
}
pvalues # none of the variables are normally distributed
```

## Correlation
```{r}
df <- subset(SD, !duplicated(SD$iid), select = c('attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'age', 'income'))
cor <- as.matrix(cor(df, use = 'complete.obs'))

lower_tri <- cor
lower_tri[upper.tri(cor)] <- NA
melted_cor <- melt(lower_tri, na.rm = T)
attr_idx <- unique(df_long$name)
attr_val <- c('Attractiveness', 'Sincerity', 'Intelligence', 'Fun', 'Ambition', 'Shared interests')

melted_cor %>% ggplot(aes(x = Var1, y = Var2, fill = value)) + geom_tile() +
  scale_fill_gradient2(limits = c(-1, 1), space = 'Lab', name="Pearson\nCorrelation", low = 'royalblue3', high = 'indianred1') +
  coord_fixed() +
  geom_text(aes(Var1, Var2, label = round(value, 2)), color = "black", size = 5) +
  theme_minimal() +
    theme(
  axis.title.x = element_blank(), 
  axis.title.y = element_blank(),
  axis.text.x = element_text(size = 11),
  axis.text.y = element_text(size = 11),
  #panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  legend.title = element_text(size = 15),
  legend.justification = c(1, 0),
  legend.position = c(0.4, 0.7),
  legend.direction = "horizontal") +
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```

## Exploratory analysis
```{r}
# Importance of each attribute by gender
attr <- subset(SD, !duplicated(SD$iid), select = c(3, 70:75)) %>%
  filter(!is.na(attr1_1))  %>% # 7 people with missing values
  group_by(gender) %>%
  summarize(Attractiveness = mean(attr1_1),
            Sincerity = mean(sinc1_1),
            Intelligence = mean(intel1_1),
            Fun = mean(fun1_1, na.rm = T),
            Ambition = mean(amb1_1, na.rm = T),
            'Shared interests' = mean(shar1_1, na.rm = T))

data_long <- pivot_longer(attr, !gender) # manipulating data to  plot

data_long %>% ggplot(aes(x = name, y = value, fill = factor(gender))) + 
  geom_bar(stat = 'identity', position = 'dodge') +
  xlab('Attribute') + ylab('Importance') + ggtitle('Importance of each attribute by gender') +
  geom_text(aes(label = round(value, 2)), position=position_dodge(width=0.9), vjust = -0.5, size = 3.2) +
  scale_fill_discrete(name="Gender",
                         breaks=c(0, 1),
                         labels=c("Women", "Men")) + # Editing legend
  theme_minimal() + # Changing theme
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size = 12)) # Centers title
```

```{r}
# Age analysis
age_df <- subset(SD, !duplicated(SD$iid)) %>%
  filter(!is.na(age)) %>%
  group_by(wave, gender) %>%
  summarize(Average_age = mean(age))

age_df$gender <- ifelse(age_df$gender == 0, 'Women', 'Men')

# Mean age per wave
age_df %>% ggplot(aes(x = wave, y = Average_age, fill = gender)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  scale_fill_discrete(name = "Gender") 

age_df <- subset(SD, !duplicated(SD$iid), select = c(iid, gender, age)) %>%
  filter(!is.na(age)) %>%
  mutate(mean = mean(age))
age_df$gender <- ifelse(age_df$gender == 0, 'Women', 'Men')

# Histogram of age
max(unique(age_df$age)) - min(unique(age_df$age)) # number of bins
age_df %>% ggplot(aes(x = age)) + 
  geom_histogram(bins = 37, position = 'identity', alpha = .7) +
  geom_vline(aes(xintercept = mean), col = 'red', linetype = 'dashed')
```

```{r}
# Field analysis
field_df <- subset(SD, !duplicated(SD$iid)) %>%
  filter(!is.na(field_cd)) %>%
  group_by(field_explained, gender) %>%
  summarize(field_sum = n())

field_df$gender <- ifelse(field_df$gender == 0, 'Women', 'Men')

field_df %>% ggplot(aes(x = field_explained, y = field_sum, fill = gender)) + 
  geom_bar(stat = 'identity', position = 'dodge') + 
  coord_flip()

```

```{r}
# Income
income_df <- subset(SD, !duplicated(SD$iid)) %>%
  filter(!is.na(income))

income_df$gender <- ifelse(income_df$gender == 0, 'Women', 'Men')

income_df %>% ggplot(aes(x = income/1000, fill = gender)) + 
  geom_histogram(bins = 15)

```

```{r}
# Purpose
goal_df <- subset(SD, !duplicated(SD$iid)) %>%
  filter(!is.na(goal)) %>%
  group_by(goal, gender) %>%
  summarise(count = n())

goal_df$gender <- ifelse(goal_df$gender == 0, 'Women', 'Men')

goal_idx <- unique(goal_df$goal)
goal_val <- c('Seemed like a fun night out', 'To meet new people', 'To get a date', 
              'Looking for a serious relationship', 'To say I did it',	'Other')
goal_df$goal_explained <- goal_val[match(goal_df$goal, goal_idx)]

goal_df %>% ggplot(aes(x = goal_explained, y = count, fill = gender)) + 
  geom_bar(stat = 'identity', position = 'dodge') +
  coord_flip()

```

## PCA
```{r}
cols <- c('gender', 'age', 'age_o', 'race_explained', 'race_explained_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr1_1', 'sinc1_1', 'intel1_1', 'amb1_1', 'shar1_1', 'attr', 'sinc', 'intel', 'amb', 'shar', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'like', 'like_o', 'prob', 'prob_o')
Y <- subset(SD, !duplicated(SD$iid), select = cols)
match <- subset(SD, !duplicated(SD$iid), select = c("match"))

asian <- ifelse(Y$race_explained == 'Asian', 1, 0)
european <- ifelse(Y$race_explained == 'European', 1, 0)
other <- ifelse(Y$race_explained == 'Other', 1, 0)
latino <- ifelse(Y$race_explained == 'Latino', 1, 0)
black <- ifelse(Y$race_explained == 'Black', 1, 0)

asian_o <- ifelse(Y$race_explained_o == 'Asian', 1, 0)
european_o <- ifelse(Y$race_explained_o == 'European', 1, 0)
other_o <- ifelse(Y$race_explained_o == 'Other', 1, 0)
latino_o <- ifelse(Y$race_explained_o == 'Latino', 1, 0)
black_o <- ifelse(Y$race_explained_o == 'Black', 1, 0)

Y <- cbind(Y, asian, european, other, latino, black, asian_o, european_o, other_o, latino_o, black_o)
match = match[complete.cases(Y), ]

Y <- Y[complete.cases(Y), ]
cols_cat <- c('asian', 'european', 'other', 'latino', 'black', 'asian_o', 'european_o', 'other_o', 'latino_o', 'black_o')

Y[, cols_cat] <- Y[, cols_cat]/sqrt(5)

colnames(Y)
Y <- Y[, -c(4, 5)] #deleting categorical variables race and race_o
Y <- Y[complete.cases(Y), ]
match = match[complete.cases(Y)]

Y_temp <- Y[, -which(colnames(Y) %in% c(cols_cat, 'gender'))]

Y_temp <- t(apply(Y_temp, 1, '-', colMeans(Y_temp, na.rm = T))) # subtracting mean

Y_temp <- t(apply(Y_temp, 1, '/', apply(Y_temp, 2, sd, na.rm = T))) # dividing by standard deviation
colnames(Y)
Y <- cbind(Y_temp, Y[, c(cols_cat, 'gender')])

Y <- t(apply(Y, 1, '-', colMeans(Y, na.rm = T)))

Y <- Y[complete.cases(Y), ]
match = match[complete.cases(Y)]

Y = Y - colMeans(Y)
s <- svd(Y[complete.cases(Y),])
diagS <- s$d
rho <- diagS^2/sum(diagS^2)

threshold = 0.9
par(mfrow=c(1, 1))
xlimits <- c(1, ncol(Y)); 
plot(rho,
     type='o',
     main="Variance explained by principal components",
     xlab="Principal component", 
     ylab="Variance explained",
     xlim=xlimits,
     ylim=c(0,1),
     col='blue')
lines(cumsum(rho), type='o', col='orange')
lines(xlimits, c(threshold, threshold), lty='dashed')
     

# scree plot
par(mfrow=c(1, 1))
eigenv = diagS^2
xlimits <- c(1, ncol(Y)); 
plot(eigenv,
     type='o',
     main="Scree plot",
     xlab="Principal component", 
     ylab="Eigenvalue",
     xlim=xlimits,
     col='blue')

##
no_match_idx = which(match == 0)
matc_idx = which(match == 1)
tmp_match = match

tmp_match[matc_idx] = "match"
tmp_match[no_match_idx] = "no_match"

tmp = as.data.frame(s$u)
tmp$match = tmp_match
no_match_idx = which(match == 0)[1:82]
matc_idx = which(match == 1)[1:82]
a = tmp[c(matc_idx, no_match_idx), ]

library(plotly)

fig <- plot_ly(a, x = ~V1, y = ~V2, z = ~V3, color = ~match, colors = c('#BF382A', '#0C4B8E'), size = I(100)) %>% add_markers() %>% layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))
  

fig

fig <- plot_ly(a, x = ~V1, y = ~V2, z = ~V3, color = ~match, colors = c('#BF382A', '#0C4B8E'), size = I(50)) 
fig <- fig %>% add_markers()
fig <- fig %>% layout(title = 'PCA-3D',
                      scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))

fig
##
eigenv = as.data.frame(s$v)
colnames(eigenv)  = colnames(Y)

#install.packages(c("FactoMineR", "factoextra"))
library("FactoMineR")
library("factoextra")

res.pca <- prcomp(Y)
p <- fviz_pca_var(res.pca)
print(p)

#install.packages("tidyverse")
set.seed(45)
corr_matrix = cor(Y)
corr_matrix

tmp = corr_matrix %*% eigen(corr_matrix)$vectors
tmp = t(tmp)[1:4, ]

pc = rep(paste0("PC", 1:4), each=38)
vals = c()
index = c()
for(row in 1:nrow(tmp)) {
  vals = c(vals, tmp[row, ])
  index = c(index, 1:38)
}

df <- data.frame(x=index, val=vals, 
                   variable=pc)
# plot
ggplot(data = df, aes(x=index, y=vals)) + geom_line(aes(colour=pc)) + xlab("Principal component") + ylab("Correlation") + scale_y_continuous(limits = c(-1, 1))
```

# Linear regression model predicting positive rate

```{r}
# percentage of getting dec_o = 1 (if a participant had 10 dates and 5 partners chose dec = 1, then positive rate = 50%)
positive_rate <- SD %>% subset(select = c('iid', 'dec', 'dec_o')) %>%
  group_by(iid) %>%
  summarise(Rate = sum(dec_o)/n())

match_rate <- subset(SD, select = c('iid', 'match', 'dec', 'dec_o')) %>%
  group_by(iid) %>%
  summarise(match_rate = mean(match)) 

means <- SD %>% subset(select = c('iid', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'like_o')) %>%
  group_by(iid) %>%
  summarise(Attr = mean(attr_o, na.rm = T),
            Sinc = mean(sinc_o, na.rm = T),
            Intel = mean(intel_o, na.rm = T),
            Amb = mean(amb_o, na.rm = T),
            Shar = mean(shar_o, na.rm = T),
            Like = mean(like_o, na.rm = T))

means_no_NA <- SD %>% subset(select = c('iid', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'like_o')) %>%
  group_by(iid) %>%
  summarise(across(attr_o:like_o, mean)) %>%
  drop_na()

dataset <- left_join(means_no_NA, positive_rate, by = 'iid')
X <- as.matrix(dataset[, c(2, 3, 5, 7)])
# Feature transformation
X <- t(apply(X, 1, '-', colMeans(X))) # subtracting mean
X <- t(apply(X, 1, '/', apply(X, 2, sd))) # dividing by standard deviation
apply(X, 2, mean)
apply(X, 2, sd)

Y <- as.matrix(dataset[, 'Rate'])

cor(X) # colinearity between Attr and Like :/

model <- glm(Y ~ X)
summary(model)

y_est <- model$fitted.values
residuals <- model$residuals

plot(Y, y_est)

# Residuals are normally distributed!
hist(Y-y_est)
shapiro.test(residuals)$p.value

# Mean squared error
mean((Y-y_est)^2)
mean((Y-y_est)^2)/mean(Y)
```

# Linear regression with R tidymodels

```{r}
library('tidymodels')

dataset <- left_join(means_no_NA, positive_rate, by = 'iid')

set.seed(123)
dataset_split <-initial_split(dataset[, c(c(2:8))], strata = 'Rate') #equal number of classes for each split
train <- training(dataset_split)
test <- testing(dataset_split)

# validation sets for tuning the model
set.seed(345)
folds <- vfold_cv(train)
folds

data_rec <- recipe(Rate ~ ., data = train) 

# Linear regression
lm_spec <- linear_reg() %>%
  set_engine('lm')

# Random forsets
rf_spec <- rand_forest(trees = 1000) %>%
  set_engine('ranger') %>%
  set_mode('regression')

#install.packages('doParallel')
#library(doParallel)
doParallel::registerDoParallel()

# Workflow - fitting on resamples
lm_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metric_set(rmse, mae, rsq_trad),
    control = control_resamples(save_pred = T) # saves predictions
  )

#library('ranger')
rf_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metric_set(rmse, mae, rsq_trad),
    control = control_resamples(save_pred = T) # saves predictions
  )

# every recipe is being executed on every fold and then random forest model is being fit on it and evaluated on the hold out set in 'splits' (resample)

collect_metrics(lm_wf)
collect_metrics(rf_wf)

# fitting on training data
final <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec) %>%
  last_fit(dataset_split)

collect_metrics(final)
collect_predictions(final)

library(tidyverse)
# plotting estimates without an intercept
final %>% pull(.workflow) %>% # similar to final$.workflow
  pluck(1) %>%
  tidy() %>%
  filter(term != '(Intercept)') %>%
  ggplot(aes(estimate, fct_reorder(term, estimate))) + geom_point(size = 4) +
  geom_vline(xintercept = 0, lty = 2) +
  geom_errorbar(aes(xmin = estimate - std.error, 
                    xmax = estimate + std.error),
                width = 0.3) + theme_minimal()
```


# Classification

## Data preparation

```{r}
#install.packages("sjmisc")
library(sjmisc)

cols <- c('iid', 'gender', 'age', 'age_o', 'race_explained', 'race_explained_o', 'samerace', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr1_1', 'sinc1_1', 'intel1_1', 'amb1_1', 'shar1_1', 'attr', 'sinc', 'intel', 'amb', 'shar', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'like', 'like_o', 'prob', 'prob_o', 'match')

df <- subset(SD, select = c('iid', 'attr', 'attr3_1', 'match')) %>%
  mutate(diff = attr3_1 - attr) %>%
  group_by(iid) %>%
  summarise(mean_diff = mean(diff, na.rm = T),
            type = case_when(mean_diff < -1.5 ~ 'underestimated',
                             mean_diff > 1.5 ~ 'overestimated',
                             TRUE ~ 'realistic')) %>%
  add_column(positive_rate = positive_rate$Rate, match_rate = match_rate$match_rate)
df

age_mean <- as.integer(mean(SD$age, na.rm = T))

data <- SD[, cols]
data <- left_join(data, df, by = 'iid') %>%
  tidyr::replace_na(list(age = age_mean, age_o = age_mean)) %>%
  mutate(age_gap = abs(age - age_o)) %>%
  subset(select = c(-age, -age_o, -iid, -type, -race_explained, -race_explained_o, -positive_rate, -match_rate)) %>%
  drop_na() %>% 
  mutate(gender = as.factor(gender),
         samerace = as.factor(samerace),
         match = as.factor(match))
  
sum(is.na(data)); dim(SD); dim(data); colnames(data)

levels(data$match) # Need to change the levels because the models view the first level as positive
data <- data %>%
  mutate(match = fct_rev(match))
```

## Building a model (KNN)

```{r}
set.seed(123)
data_split <- initial_split(data, strata = match)
train <- training(data_split)
test <- testing(data_split)

set.seed(456)
folds <- vfold_cv(train)
folds

# Defining a recipe
data_rec <- recipe(match ~ ., data = train) %>%
  step_normalize(all_numeric())

# Executing the recipe
norm <- data_rec %>% prep() %>% juice()
norm %>% select(where(is.numeric)) %>% sapply(mean)
norm %>% select(where(is.numeric)) %>% sapply(sd)

# Selecting metrics
c_metrics <- metric_set(accuracy, sens, roc_auc, mn_log_loss)

# Saving predictions
model_control <- control_grid(save_pred = T)

# Defining a model
#install.packages('kknn')
library(kknn)
knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_mode('classification') %>%
  set_engine('kknn')

knn_grid <- grid_regular(parameters(knn_spec), levels = 5)

knn_tune <- tune_grid(
  knn_spec,
  data_rec,
  resamples = folds,
  control = model_control,
  metrics = c_metrics
)

# Plotting metrics
collect_metrics(knn_tune) %>%
  ggplot(aes(x = neighbors, y = mean)) + 
  geom_point() + geom_line() + 
  facet_wrap(~.metric, scales = 'free_y')

# to see variance between each fold
knn_tune %>% select(id, .metrics) %>%
  unnest(.metrics) %>%
  ggplot(aes(x = neighbors, y = .estimate, color = id)) + geom_point() + geom_line() + 
  facet_wrap(~.metric, scales = 'free_y')

data_metrics <- knn_tune %>% collect_predictions() %>%
  mutate(pred = if_else(.pred_1 >= .5, 1, 0),
         pred = as.factor(pred),
         pred = fct_rev(pred)) 

data_metrics %>%
  conf_mat(match, pred) #quite a lot of false negatives :/

data_metrics %>% accuracy(match, pred)
data_metrics %>% sens(match, pred) # pooor sensitivity
data_metrics %>% ppv(match, pred) 

# roc curve
knn_tune %>% collect_predictions() %>%
  group_by(id) %>%
  roc_curve(match, .pred_1) %>%
  autoplot()

# gain curve - how much of the class I encapture given the class probabilities, that is, if we say that everyone with the 50% prob. or less being a "match" (1) class, we can capture around 80% of the actual points
knn_tune %>% 
  collect_predictions() %>%
  gain_curve(match, .pred_1) %>%
  autoplot()
  
# Collect metrics and fit to a model
knn_tune %>% select_best(metric = 'roc_auc')
knn_tune %>% select_best(metric = 'accuracy')

# Defining a final model
knn_spec_final <- nearest_neighbor(neighbors = 14) %>%
  set_mode('classification') %>%
  set_engine('kknn')

# Defining a workflow
final_model <- workflow() %>%
  add_model(knn_spec_final) %>%
  add_recipe(data_rec)

# Fitting a final model
final_results <- last_fit(final_model, data_split)

# if we say 50% of these people got match, around 50% of those people actually got match
final_results %>% collect_predictions() %>%
  select(.pred_1, match) %>%
  mutate(.pred = 100*.pred_1) %>%
  select(-.pred_1) %>%
  mutate(.pred = round(.pred/5)*5) %>% #rounding to 5
  count(.pred, match) %>%
  pivot_wider(names_from = match, values_from = n) %>%
  rename(Yes = `1`, No = `0`) %>%
  mutate(prob = Yes/(Yes + No)) %>%
  mutate(prob = prob*100) %>%
  ggplot(aes(x = .pred, y = prob)) + geom_point() + geom_smooth() + geom_abline() +
  coord_fixed(ylim = c(0,100), xlim = c(0, 100))
```

# Place to work on different models

```{r}

rf_ranger_spec <- rand_forest(trees = 1000, mode = "classification") %>%
  set_engine('ranger')

rf_rf_spec <- rand_forest(trees = 1000, mode = "classification") %>%
  set_engine('random_forest')

lg_spec <- logistic_reg(mode = "classification") %>%
  set_engine('glm')

# Defining workflows
rf_ranger_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_ranger_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metric_set(),
    control = control_resamples(save_pred = T)

```










```{r}
cols <- c('gender', 'age', 'age_o', 'race_explained', 'race_explained_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr1_1', 'sinc1_1', 'intel1_1', 'amb1_1', 'shar1_1', 'attr', 'sinc', 'intel', 'amb', 'shar', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'like', 'like_o', 'prob', 'prob_o', 
          'attr3_1', 'sinc3_1', 'intel3_1', 'amb3_1')

df <- subset(SD, select = cols)
dim(df)
df <- df[complete.cases(df),]

df %>% count(race_explained, gender)

subset(SD, select = c('iid', 'attr', 'attr3_1', 'match')) %>%
  mutate(type = case_when(attr3_1 < 5 ~ 'underestimated',
                          attr3_1 > 7 ~ 'overestimated',
                          TRUE ~ 'realistic'),
         diff = attr3_1 - attr) %>%
  #ggplot(aes(diff, fill = type)) + geom_histogram(bins = 20)
  group_by(iid) %>%
  summarise(mean_attr_rating = mean(attr),
            match_rate = sum(match)/n(),
            attr3_1 = unique(attr3_1)) %>%
  mutate(type = case_when(attr3_1 - mean_attr_rating < -1.5 ~ 'underestimated',
                          attr3_1 - mean_attr_rating > 1.5 ~ 'overestimated',
                          TRUE ~ 'realistic')) %>%
  ggplot(aes(mean_attr_rating, match_rate, color = type)) + geom_point(pch = 1, size = 3)
  
  summarise(match_rate = sum(match)/n(),
            attr3_1 = unique(attr3_1)) %>%
  ggplot(aes(attr3_1, match_rate)) + geom_col()

head(SD, 50)

```