in---
title: "SD_project2"
author: "Katarzyna Otko"
date: "3 11 2020"
output: html_document
---

```{r}
library(data.table)    # provides enhanced data.frame
library(ggplot2)       # plotting
library(glmnet)        # ridge, elastic net, and lasso 
library(R.utils)
library(tidymodels)

SampleWeight <- function(target, weights) {
  weight_per_sample = target
  weight_per_sample[weight_per_sample == 0] = weights[1]
  weight_per_sample[weight_per_sample == 1] = weights[2]
  return (weight_per_sample)
}

# data loading

data <- read.csv('/home/nomow/Documents/DTU/Intro_to_machine_learning/ML_speed_dating/SD_clean.csv')

rm_cols = c("X","iid", "id", "idg", "condtn", "wave", "round", "position",  "order", "partner", "pid", "field", "field_cd",  "race", "from", "zipcode", "career", "career_c", "dec", "match_es", "satis_2", "length", "numdat_2", 'race', 'race_o', 'field_cd', 'field', 'int_corr', "dec_o", "race_explained","race_explained_o", "field_explained", "met", "met_o", "like", "like_o")
data = data[ , -which(names(data) %in% rm_cols)]

feature_names = c("age", "age_o", "go_out", "sports", "tvsports", "exercise", "dining", "museums", "art", "hiking","gaming", "clubbing", "reading", "tv", "theater", "movies", "concerts", "music", "shopping", "yoga", 
 "exphappy", "go_out_o", "sports_o", "tvsports_o", "exercise_o", "dining_o", "museums_o", "art_o", "hiking_o", "gaming_o", "clubbing_o",
 "reading_o", "tv_o", "theater_o", "mvies_o", "concerts_o", "music_o", "shopping_o", "yoga_o", "exphappy_o")

```


```{r}
## Feature selection
x = (data[, -2])
x = scale(x)
y = (data[, 2])

l1_net <- cv.glmnet(data.matrix(x), data.matrix(y), family="binomial", alpha=1)
best_lambda_lasso <- l1_net$lambda.1se  # largest lambda in 1 SE
lasso_coef <- l1_net$glmnet.fit$beta[,  # retrieve coefficients
              l1_net$glmnet.fit$lambda  # at lambda.1se
              == best_lambda_lasso]
features_to_use = unique(c(feature_names, colnames(x)[which(lasso_coef != 0)]))

x = x[ , which(colnames(x) %in% features_to_use)]

```

```{r}

## sample weighting
weights_per_class = 1 / (table(y) / length(y))



# nested cross-validation
transformed_data = as.data.frame(cbind(x, match=y))
outer_nb_folds = 10
inner_nb_folds = 10
nb_models = 10
folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(strata = match,  v = outer_nb_folds), 
                     inside = vfold_cv(strata = match,  v = inner_nb_folds))




# outer fold
for (i in 1:outer_nb_folds) {
  # divides in outer train/test

  outer_data = folds$splits[[i]]$data
  idx_outer_train = folds$splits[[i]]$in_id
  idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
  outer_train = outer_data[idx_outer_train, ]
  outer_test = outer_data[idx_outer_test, ]

  # inner fold
  for (j in 1:inner_nb_folds) {
    # divides in inner train/test
    inner_data = folds$inner_resamples[[i]]$splits[[j]]$data
    idx_inner_train = folds$inner_resamples[[i]]$splits[[j]]$in_id
    idx_inner_test = -which(as.numeric(rownames(inner_data)) %in% idx_inner_train)
    inner_train = inner_data[idx_inner_train, ]
    inner_test = inner_data[idx_inner_train, ]
    
    # model train
    for (k in 1:nb_models) {
      weights_inner = SampleWeight(inner_train[, ncol(inner_train)], weights_per_class)
      
    }
  }

  weights_outer = SampleWeight(outer_train[, ncol(outer_train)], weights_per_class)

}



```



```{r}

## feature extracton



#install.packages("sjmisc")
library(sjmisc)

cols <- c('iid', 'gender', 'age', 'age_o', 'race_explained', 'race_explained_o', 'samerace', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr1_1', 'sinc1_1', 'intel1_1', 'amb1_1', 'shar1_1', 'attr', 'sinc', 'intel', 'amb', 'shar', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'like', 'like_o', 'prob', 'prob_o', 'match')

df <- subset(SD, select = c('iid', 'attr', 'attr3_1', 'match')) %>%
  mutate(diff = attr3_1 - attr) %>%
  group_by(iid) %>%
  summarise(mean_diff = mean(diff, na.rm = T),
            type = case_when(mean_diff < -1.5 ~ 'underestimated',
                             mean_diff > 1.5 ~ 'overestimated',
                             TRUE ~ 'realistic')) %>%
  add_column(positive_rate = positive_rate$Rate, match_rate = match_rate$match_rate)
df

age_mean <- as.integer(mean(SD$age, na.rm = T))

data <- SD[, cols]
data <- left_join(data, df, by = 'iid') %>%
  tidyr::replace_na(list(age = age_mean, age_o = age_mean)) %>%
  mutate(age_gap = abs(age - age_o)) %>%
  subset(select = c(-age, -age_o, -iid, -type, -race_explained, -race_explained_o, -positive_rate, -match_rate)) %>%
  drop_na() %>% 
  mutate(gender = as.factor(gender),
         samerace = as.factor(samerace),
         match = as.factor(match))
  
sum(is.na(data)); dim(SD); dim(data); colnames(data)

levels(data$match) # Need to change the levels because the models view the first level as positive
data <- data %>%
  mutate(match = fct_rev(match))

```

## Building a model (KNN)

```{r}
set.seed(456)
data_split <- initial_split(data, strata = match)
train <- training(data_split)
test <- testing(data_split)

set.seed(456)
folds <- vfold_cv(train, strata = match)
folds

# Defining a recipe
data_rec <- recipe(match ~ ., data = train) %>%
  step_normalize(all_numeric())

# Executing the recipe
norm <- data_rec %>% prep() %>% juice()
norm %>% select(where(is.numeric)) %>% sapply(mean)
norm %>% select(where(is.numeric)) %>% sapply(sd)

# Selecting metrics
c_metrics <- metric_set(accuracy, sens, roc_auc, mn_log_loss)

# Saving predictions
model_control <- control_grid(save_pred = T)

# Defining a model
#install.packages('kknn')
library(kknn)
knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_mode('classification') %>%
  set_engine('kknn')

knn_grid <- grid_regular(parameters(knn_spec), levels = 5)

knn_tune <- tune_grid(
  knn_spec,
  data_rec,
  resamples = folds,
  control = model_control,
  metrics = c_metrics
)

# Plotting metrics
collect_metrics(knn_tune) %>%
  ggplot(aes(x = neighbors, y = mean)) + 
  geom_point() + geom_line() + 
  facet_wrap(~.metric, scales = 'free_y')

# to see variance between each fold
knn_tune %>% select(id, .metrics) %>%
  unnest(.metrics) %>%
  ggplot(aes(x = neighbors, y = .estimate, color = id)) + geom_point() + geom_line() + 
  facet_wrap(~.metric, scales = 'free_y')

data_metrics <- knn_tune %>% collect_predictions() %>%
  mutate(pred = if_else(.pred_1 >= .5, 1, 0),
         pred = as.factor(pred),
         pred = fct_rev(pred)) 

data_metrics %>%
  group_by(id) %>%
  summarise()

data_metrics %>%
  conf_mat(match, pred) #quite a lot of false negatives :/

data_metrics %>% accuracy(match, pred)
data_metrics %>% sens(match, pred) # pooor sensitivity
data_metrics %>% ppv(match, pred) 

# roc curve
knn_tune %>% collect_predictions() %>%
  group_by(id) %>%
  roc_curve(match, .pred_1) %>%
  autoplot()

# gain curve - how much of the class I encapture given the class probabilities, that is, if we say that everyone with the 50% prob. or less being a "match" (1) class, we can capture around 80% of the actual points
knn_tune %>% 
  collect_predictions() %>%
  gain_curve(match, .pred_1) %>%
  autoplot()
  
# Collect metrics and fit to a model
knn_tune %>% select_best(metric = 'roc_auc')
knn_tune %>% select_best(metric = 'accuracy')

# Defining a final model
knn_spec_final <- nearest_neighbor(neighbors = 14) %>%
  set_mode('classification') %>%
  set_engine('kknn')

# Defining a workflow
final_model <- workflow() %>%
  add_model(knn_spec_final) %>%
  add_recipe(data_rec)

# Fitting a final model
final_results <- last_fit(final_model, data_split)

# if we say 50% of these people got match, around 50% of those people actually got match
final_results %>% collect_predictions() %>%
  select(.pred_1, match) %>%
  mutate(.pred = 100*.pred_1) %>%
  select(-.pred_1) %>%
  mutate(.pred = round(.pred/5)*5) %>% #rounding to 5
  count(.pred, match) %>%
  pivot_wider(names_from = match, values_from = n) %>%
  rename(Yes = `1`, No = `0`) %>%
  mutate(prob = Yes/(Yes + No)) %>%
  mutate(prob = prob*100) %>%
  ggplot(aes(x = .pred, y = prob)) + geom_point() + geom_smooth() + geom_abline() +
  coord_fixed(ylim = c(0,100), xlim = c(0, 100))

```















