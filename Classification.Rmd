---
title: "SD_project2"
author: "Katarzyna Otko"
date: "3 11 2020"
output: html_document
---

```{r}
library(data.table)    # provides enhanced data.frame
library(ggplot2)       # plotting
library(glmnet)        # ridge, elastic net, and lasso 
library(FNN)           # KNN
library(R.utils)
library(tidymodels)


# helper functions
SampleWeight <- function(target, weights) {
  weight_per_sample = target
  weight_per_sample[weight_per_sample == 0] = weights[1]
  weight_per_sample[weight_per_sample == 1] = weights[2]
  return (weight_per_sample)
}


l2_reg <- function(train_source, train_target, test_source, test_target, lambda, weights, tresh) {

    # train model
    net <- glmnet(data.matrix(train_source), 
                        data.matrix(train_target), 
                        alpha = 0,                             
                        lambda = lambda, 
                        weights = weights,
                        family="binomial")
    
    # train error
    train_res = predict(net, data.matrix(train_source) ,s=lambda)
    train_res[train_res < tresh] = 0
    train_res[train_res >= tresh] = 1
    train_error = 1 - (sum(train_res == train_target) / length(train_target))
    
    # test error
    test_res = predict(net, data.matrix(test_source) ,s=lambda)
    test_res[test_res < tresh] = 0
    test_res[test_res >= tresh] = 1
    test_error = 1 - (sum(test_res == test_target) / length(test_target))
    return (list(train_error, test_error, test_res))
}


######################################################
# data loading
#####################################################
# Alvils
data <- read.csv('/home/nomow/Documents/DTU/Intro_to_machine_learning/ML_speed_dating/SD_clean.csv')
# Kasia
setwd("C:/Users/katin/Desktop/Folder/STUDIA/DTU/Semestr I/Intro to ML/ML_speed_dating")
data <- read.csv('SD_clean.csv')

# removes columns that are irrevelant
rm_cols = c("X","iid", "id", "idg", "condtn", "wave", "round", "position",  "order", "partner", "pid", "field", "field_cd",  "race", "from", "zipcode", "career", "career_c", "dec", "match_es", "satis_2", "length", "numdat_2", 'race', 'race_o', 'field_cd', 'field', 'int_corr', "dec_o", "race_explained","race_explained_o", "field_explained", "met", "met_o", "like", "like_o")
data = data[ , -which(names(data) %in% rm_cols)]


######################################################
## Feature selection
######################################################
feature_names = c("age", "age_o", "go_out", "sports", "tvsports", "exercise", "dining", "museums", "art", "hiking","gaming", "clubbing", "reading", "tv", "theater", "movies", "concerts", "music", "shopping", "yoga", 
 "exphappy", "go_out_o", "sports_o", "tvsports_o", "exercise_o", "dining_o", "museums_o", "art_o", "hiking_o", "gaming_o", "clubbing_o",
 "reading_o", "tv_o", "theater_o", "mvies_o", "concerts_o", "music_o", "shopping_o", "yoga_o", "exphappy_o")
# data normalization
x = (data[, -2])
x = scale(x)
y = (data[, 2])

# l1 logistic regression model, where we choose features which weights are not 0
l1_net <- cv.glmnet(data.matrix(x), data.matrix(y), family="binomial", alpha=1)
best_lambda_lasso <- l1_net$lambda.1se  # largest lambda in 1 SE
lasso_coef <- l1_net$glmnet.fit$beta[,  # retrieve coefficients
              l1_net$glmnet.fit$lambda  # at lambda.1se
              == best_lambda_lasso]

# adds features to use from manual selection and automatic
features_to_use = unique(c(feature_names, colnames(x)[which(lasso_coef != 0)]))
x = x[ , which(colnames(x) %in% features_to_use)]


colnames(l1_net$glmnet.fit$beta)
l1_net$lambda
sort(round(lasso_coef, 2))


```


# Regularized logistic regression

```{r}
##########################
## L2 logistic regression
##########################

## sample weighting
weights_per_class = 1 / (table(y) / length(y))

# nested cross-validation
transformed_data = as.data.frame(cbind(x, match=y))
outer_nb_folds = 10
inner_nb_folds = 10
tresh = 0.5
params = seq(0, 1, by = 0.1)
nb_params = length(params)
best_model_hyperparam = rep(0, (nb_params - 1))
error_per_fold = rep(0, outer_nb_folds)

y_hat_lr <- rep(NA, nrow(transformed_data))

#################
# nested crossval
#################

set.seed(123)
# creates nested crossvalidation both inside and outside fold is stratified
folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(strata = match,  v = outer_nb_folds), 
                     inside = vfold_cv(strata = match,  v = inner_nb_folds))


# outer fold 
for (i in 1:outer_nb_folds) {
  # divides in outer train/test
  outer_data = folds$splits[[i]]$data
  idx_outer_train = folds$splits[[i]]$in_id
  idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
  outer_train = outer_data[idx_outer_train, ]
  outer_test = outer_data[idx_outer_test, ]
  inner_error = matrix(0L, nrow = inner_nb_folds, ncol = nb_params)
  
  # inner fold
  for (j in 1:inner_nb_folds) {
    # divides in inner train/test
    inner_data = folds$inner_resamples[[i]]$splits[[j]]$data
    idx_inner_train = folds$inner_resamples[[i]]$splits[[j]]$in_id
    idx_inner_test = -which(as.numeric(rownames(inner_data)) %in% idx_inner_train)
    inner_train = inner_data[idx_inner_train, ]
    inner_test = inner_data[idx_inner_test, ]
    
    # hyperparam optim
    for (k in 1:nb_params) {
      weights_inner = SampleWeight(inner_train[, ncol(inner_train)], weights_per_class)
      # trains model and gets train and test error
      error = l2_reg(inner_train[, -ncol(inner_train)],
                     inner_train[, ncol(inner_train)],
                     inner_test[, -ncol(inner_test)],
                     inner_test[, ncol(inner_test)],
                     params[k],
                     weights_inner, 
                     tresh)
      # inner error for each inner fold
      inner_error[j, k] = error[[2]]
    }
  }
  
  # calculates avg error for each model in inner fold and trains once more where the generalization error is minimum
  min_idx = which.min(colMeans(inner_error))
  best_model_hyperparam[i] = min_idx
  weights_outer = SampleWeight(outer_train[, ncol(outer_train)], weights_per_class)
  error = l2_reg(outer_train[, -ncol(outer_train)],
                 outer_train[, ncol(outer_train)],
                 outer_test[, -ncol(outer_test)],
                 outer_test[, ncol(outer_test)],
                 params[min_idx],
                 weights_outer, 
                 tresh)
  error_per_fold[i] = error[[2]]
  # obtains predictions for McNemar test
  y_hat_lr[idx_outer_test] <- error[[3]]

}

# for table for project 2
l2_gen_error = mean(error_per_fold); l2_gen_error
l2_error_per_fold = error_per_fold; l2_error_per_fold
l2_params = params; l2_params
l2_best_hyperparam_per_fold = best_model_hyperparam; l2_best_hyperparam_per_fold

######################################
## final param selection using crossval
######################################
weights = SampleWeight(y, weights_per_class)
l2_net <- cv.glmnet(data.matrix(x), data.matrix(y), family="binomial", alpha=0, lambda = params, weights = weights)
best_lambda_l2 <- l2_net$lambda.1se  # largest lambda in 1 SE
best_lambda_l2

ridge_coef <- l2_net$glmnet.fit$beta[,  # retrieve coefficients
              l2_net$glmnet.fit$lambda  # at lambda.1se
              == best_lambda_l2]

ridge_coef <- sort(ridge_coef, decreasing = T) %>% as.data.frame()
colnames(ridge_coef) <- 'Coefficient'

ridge_coef[order(Coefficient),]

ridge_coef  %>%
  ggplot(aes(y = reorder(rownames(ridge_coef), Coefficient), x = Coefficient)) + 
  geom_point() + 
  geom_vline(xintercept = 0, lty = 2) +
  ylab("Feature") + ggtitle("Logistic regression coefficients") +
  theme_minimal() + # Changing theme
  theme(plot.title = element_text(hjust = 0.5), # Centers title
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12), # x/y labels position
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size = 12))

coeffs <- sort(rowMeans(l2_net$glmnet.fit$beta), decreasing = TRUE)
coeffs_to_plot <- coeffs[c(1:8, 57)] %>% as.data.frame()
coeffs_to_plot 

coeffs_to_plot %>% ggplot(aes(y = rownames(coeffs_to_plot), x = .)) + geom_point()


length(coeffs)



```


# KNN

```{r}

# nested cross-validation
transformed_data = as.data.frame(cbind(x, match=y))
k_params <- c(1, 3, 5, 7)
nb_params <- length(k_params)
outer_nb_folds <- 10
inner_nb_folds <- 10
best_model_hyperparam = rep(0, (nb_params - 1))
error_per_fold = rep(0, outer_nb_folds)

y_hat_knn <- rep(NA, nrow(transformed_data))

#################
# nested crossval
#################
set.seed(123)
# creates nested crossvalidation both inside and outside fold is stratified
folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(strata = match,  v = outer_nb_folds), 
                     inside = vfold_cv(strata = match,  v = inner_nb_folds))


# outer fold 
for (i in 1:outer_nb_folds) {
  # divides in outer train/test
  outer_data = folds$splits[[i]]$data
  idx_outer_train = folds$splits[[i]]$in_id
  idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
  outer_train = outer_data[idx_outer_train, ]
  outer_test = outer_data[idx_outer_test, ]
  inner_error = matrix(0L, nrow = inner_nb_folds, ncol = nb_params)
  
  # inner fold
  for (j in 1:inner_nb_folds) {
    # divides in inner train/test
    inner_data = folds$inner_resamples[[i]]$splits[[j]]$data
    idx_inner_train = folds$inner_resamples[[i]]$splits[[j]]$in_id
    idx_inner_test = -which(as.numeric(rownames(inner_data)) %in% idx_inner_train)
    inner_train = inner_data[idx_inner_train, ]
    inner_test = inner_data[idx_inner_test, ]
    
    # hyperparam optim
    for (k in 1:nb_params) {
      x_inner_train <- inner_train[, 1:(ncol(inner_train) - 1)]
      y_inner_train <- inner_train[, 'match']
      x_inner_test <- inner_test[, 1:(ncol(inner_train) - 1)]
      y_inner_test <- inner_test[, 'match']
      
      # trains model and gets a test error
      y_pred <- knn(x_inner_train, 
                    x_inner_test, 
                    y_inner_train,
                    k = k_params[k],
                    algorithm = "kd_tree")
      
      # inner error for each inner fold
      inner_error[j, k] <- 1 - (sum(y_pred == y_inner_test) / length(y_inner_test))
    }
  }
  
  # calculates avg error for each model in inner fold and trains once more where the generalization error is minimum
  min_idx = which.min(colMeans(inner_error))
  best_model_hyperparam[i] = min_idx
  
  x_outer_train <- outer_train[, 1:(ncol(outer_train) - 1)]
  y_outer_train <- outer_train[, 'match']
  x_outer_test <- outer_test[, 1:(ncol(outer_train) - 1)]
  y_outer_test <- outer_test[, 'match']
      
  # trains model and gets a test error
  y_pred <- knn(x_outer_train, 
                x_outer_test, 
                y_outer_train,
                k = k_params[min_idx],
                algorithm = "kd_tree")
  error_per_fold[i] <- 1 - (sum(y_pred == y_outer_test) / length(y_outer_test))
  
  # obtains predictions for McNemar test
  y_hat_knn[idx_outer_test] <- as.numeric(as.character(y_pred))

}

table(y_pred, y_outer_test)

knn_gen_error = mean(error_per_fold); knn_gen_error
knn_error_per_fold = error_per_fold; knn_error_per_fold
knn_params = k_params; knn_params
knn_best_hyperparam_per_fold = k_params[best_model_hyperparam]; knn_best_hyperparam_per_fold

######################################
## final param selection using crossval
######################################
weights = SampleWeight(y, weights_per_class)
l2_net <- cv.glmnet(data.matrix(x), data.matrix(y), family="binomial", alpha=0, lambda = params, weights = weights)
best_lambda_l2 <- l2_net$lambda.1se 


dim(inner_train)

```


# Baseline

```{r}

base <- function(train_source, train_target, test_source, test_target) {
    train_counts <- as.data.frame(table(train_target))
    most_frequent_class <- train_counts[which.max(train_counts$Freq), 'train_target'] %>% as.character() %>% as.numeric()
    # train error
    train_error = 1 - (sum(train_target == most_frequent_class) / length(train_target))
    # test error
    test_error = 1 - (sum(test_target == most_frequent_class) / length(test_target))
    return (list(train_error, test_error, most_frequent_class))
}


outer_nb_folds = 10
inner_nb_folds = 10
params = c(1)
nb_params = length(params)
best_model_hyperparam = rep(0, (nb_params - 1))
error_per_fold = rep(0, outer_nb_folds)

y_hat_base <- rep(NA, nrow(transformed_data))

set.seed(123)
# creates nested crossvalidation both inside and outside fold is stratified
folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(v = outer_nb_folds), 
                     inside = vfold_cv(v = inner_nb_folds))


# outer fold 
for (i in 1:outer_nb_folds) {
  # divides in outer train/test
  outer_data = folds$splits[[i]]$data
  idx_outer_train = folds$splits[[i]]$in_id
  idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
  outer_train = outer_data[idx_outer_train, ]
  outer_test = outer_data[idx_outer_test, ]
  inner_error = matrix(0L, nrow = inner_nb_folds, ncol = nb_params)
  
  # inner fold
  for (j in 1:inner_nb_folds) {
    # divides in inner train/test
    inner_data = folds$inner_resamples[[i]]$splits[[j]]$data
    idx_inner_train = folds$inner_resamples[[i]]$splits[[j]]$in_id
    idx_inner_test = -which(as.numeric(rownames(inner_data)) %in% idx_inner_train)
    inner_train = inner_data[idx_inner_train, ]
    inner_test = inner_data[idx_inner_test, ]
    # hyperparam optim
    for (k in 1:nb_params) {
      # trains model and gets train and test error
      error = base(inner_train[, -ncol(inner_train)],
                     inner_train[, ncol(inner_train)],
                     inner_test[, -ncol(inner_test)],
                     inner_test[, ncol(inner_test)])
      # inner error for each inner fold
      inner_error[j, k] = error[[2]]
    }
  }
  
  # calculates avg error for each model in inner fold and trains once more where the generalization error is minimum
  min_idx = which.min(colMeans(inner_error))
  best_model_hyperparam[i] = min_idx
  #class0_to_1 <- 
  
  error = base(outer_train[, -ncol(outer_train)],
                 outer_train[, ncol(outer_train)],
                 outer_test[, -ncol(outer_test)],
                 outer_test[, ncol(outer_test)])
  error_per_fold[i] = error[[2]]

  # obtains predictions for McNemar test
  y_hat_base[idx_outer_test] <- rep(error[[3]], length(outer_test[, ncol(outer_test)]))

}

# for table for project 2
base_gen_error = mean(error_per_fold); base_gen_error
base_error_per_fold = error_per_fold; base_error_per_fold
base_params = params; base_params
base_best_hyperparam_per_fold = best_model_hyperparam; base_best_hyperparam_per_fold

```


# Statistical evaluation

```{r}
source("C:/Users/katin/Desktop/Folder/STUDIA/DTU/Semestr I/Intro to ML/02450Toolbox_R/setup.R")

models <- list(y_hat_lr, y_hat_knn, y_hat_base)
alpha <- c(0.01, 0.05, 0.1)
pvalues <- rep(NA, 3)
names(pvalues) <- c('lr_knn', 'lr_base', 'knn_base')
CIs <- list()
for (a in 1:3) {
  CIs[[a]] <- matrix(NA, 3, 2); rownames(CIs[[a]]) = c('lr_knn', 'lr_base', 'knn_base')
  rt_lr_knn <- mcnemar(transformed_data[,'match'], models[[1]], models[[2]], alpha=alpha[a])
  pvalues[1] <- rt_lr_knn$p
  CIs[[a]][1,] <- rt_lr_knn$CI
  
  rt_lr_base <- mcnemar(transformed_data[,'match'], models[[1]], models[[3]], alpha=alpha[a])
  pvalues[2] <- rt_lr_base$p
  CIs[[a]][2,] <- rt_lr_base$CI
  
  rt_knn_base <- mcnemar(transformed_data[,'match'], models[[2]], models[[3]], alpha=alpha[a])
  pvalues[3] <- rt_knn_base$p
  CIs[[a]][3,] <- rt_knn_base$CI

}
names(CIs) <- c('alpha = 0.01', 'alpha = 0.05', 'alpha = 0.1')
CIs
round(pvalues, 3)

```



# Unnecessary stuff

```{r}
transformed_data = as.data.frame(cbind(x, match=y))

# KNN
k_params <- c(1)
nb_params <- length(k_params)
outer_nb_folds <- 10
inner_nb_folds <- 10
error_per_param = rep(0, nb_params)
Error_test_knn <- matrix(nrow = nb_params, ncol = outer_nb_folds)
y_hat_knn <- rep(NA, nrow(transformed_data))

set.seed(123)

folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(strata = match,  v = outer_nb_folds), 
                     inside = vfold_cv(strata = match,  v = inner_nb_folds))

for (k in 1:nb_params) {
  
    for (i in 1:outer_nb_folds) {

      outer_data = folds$splits[[i]]$data
      idx_outer_train = folds$splits[[i]]$in_id
      idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
      outer_train = outer_data[idx_outer_train, ]
      outer_test = outer_data[idx_outer_test, ]
  
      x_outer_train <- outer_train[, 1:(ncol(outer_train) - 1)]
      y_outer_train <- outer_train[, 'match']
      x_outer_test <- outer_test[, 1:(ncol(outer_train) - 1)]
      y_outer_test <- outer_test[, 'match']
          
      y_pred <- knn(x_outer_train, 
                    x_outer_test, 
                    y_outer_train,
                    k = k_params[k],
                    algorithm = "kd_tree")
      y_hat_knn[idx_outer_test] <- as.numeric(as.character(y_pred))
      
      Error_test_knn[k, i] <- 1 - (sum(y_pred == y_outer_test) / length(y_outer_test))
   
    }
}

# Regularized LR
lr_params = 0.3
nb_params <- length(lr_params)
tresh = 0.5
Error_test_lr <- matrix(nrow = nb_params, ncol = outer_nb_folds)
y_hat_lr <- rep(NA, nrow(transformed_data))
weights_per_class = 1 / (table(y) / length(y))

set.seed(123)

folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(strata = match,  v = outer_nb_folds), 
                     inside = vfold_cv(strata = match,  v = inner_nb_folds))


for (k in 1:nb_params) {
  
    for (i in 1:outer_nb_folds) {

      outer_data = folds$splits[[i]]$data
      idx_outer_train = folds$splits[[i]]$in_id
      idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
      outer_train = outer_data[idx_outer_train, ]
      outer_test = outer_data[idx_outer_test, ]
  
      x_outer_train <- outer_train[, 1:(ncol(outer_train) - 1)]
      y_outer_train <- outer_train[, 'match']
      x_outer_test <- outer_test[, 1:(ncol(outer_train) - 1)]
      y_outer_test <- outer_test[, 'match']
      
      weights_outer = SampleWeight(outer_train[, ncol(outer_train)], weights_per_class)

      error = l2_reg(outer_train[, -ncol(outer_train)],
                     outer_train[, ncol(outer_train)],
                     outer_test[, -ncol(outer_test)],
                     outer_test[, ncol(outer_test)],
                     lr_params[k],
                     weights_outer, 
                     tresh)
      y_hat_lr[idx_outer_test] <- error[[3]]
      
      Error_test_lr[k, i] <- error[[2]]
   
    }
}


# Baseline

base_params = 1
nb_params <- length(base_params)
Error_test_base <- matrix(nrow = nb_params, ncol = outer_nb_folds)
y_hat_base <- rep(NA, nrow(transformed_data))

for (k in 1:nb_params) {
  
    for (i in 1:outer_nb_folds) {

      outer_data = folds$splits[[i]]$data
      idx_outer_train = folds$splits[[i]]$in_id
      idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
      outer_train = outer_data[idx_outer_train, ]
      outer_test = outer_data[idx_outer_test, ]
      #error = matrix(0L, nrow = outer_nb_folds, ncol = nb_params)
  
      x_outer_train <- outer_train[, 1:(ncol(outer_train) - 1)]
      y_outer_train <- outer_train[, 'match']
      x_outer_test <- outer_test[, 1:(ncol(outer_train) - 1)]
      y_outer_test <- outer_test[, 'match']
          
      # trains model and gets a test error
      
      error = base(outer_train[, -ncol(outer_train)],
                     outer_train[, ncol(outer_train)],
                     outer_test[, -ncol(outer_test)],
                     outer_test[, ncol(outer_test)])
      # inner error for each inner fold
      Error_test_base[k, i] = error[[2]]
      y_hat_base[idx_outer_test] <- rep(0, length(outer_test[, ncol(outer_test)]))
   
    }
}



length(y_hat_base)
```



# KNN one-layer CV

```{r}
k_params <- c(3)
nb_params <- length(k_params)
outer_nb_folds <- 10
inner_nb_folds <- 10
best_model_hyperparam = rep(0, (nb_params - 1))
error_per_param = rep(0, nb_params)

Error_test <- matrix(nrow = nb_params, ncol = outer_nb_folds)


set.seed(123)
# creates nested crossvalidation both inside and outside fold is stratified
folds <- nested_cv(transformed_data, 
                     outside = vfold_cv(strata = match,  v = outer_nb_folds), 
                     inside = vfold_cv(strata = match,  v = inner_nb_folds))


# outer fold 
for (k in 1:nb_params) {
  
    for (i in 1:outer_nb_folds) {

      outer_data = folds$splits[[i]]$data
      idx_outer_train = folds$splits[[i]]$in_id
      idx_outer_test = -which(as.numeric(rownames(outer_data)) %in% idx_outer_train)
      outer_train = outer_data[idx_outer_train, ]
      outer_test = outer_data[idx_outer_test, ]
      #error = matrix(0L, nrow = inner_nb_folds, ncol = nb_params)
  
      x_outer_train <- outer_train[, 1:(ncol(outer_train) - 1)]
      y_outer_train <- outer_train[, 'match']
      x_outer_test <- outer_test[, 1:(ncol(outer_train) - 1)]
      y_outer_test <- outer_test[, 'match']
          
      # trains model and gets a test error
      y_pred <- knn(x_outer_train, 
                    x_outer_test, 
                    y_outer_train,
                    k = k_params[k],
                    algorithm = "kd_tree")
      
      Error_test[k, i] <- 1 - (sum(y_pred == y_outer_test) / length(y_outer_test))
   
    }
}

table(y_pred, y_outer_test)

result <- rowMeans(Error_test)
plot(k_params, result, type = 'l', xlab = 'K', ylab = 'Test error')


  
  # calculates avg error for each model in inner fold and trains once more where the generalization error is minimum
  min_idx = which.min(error_per_param)
  best_model_hyperparam[i] = min_idx
  


}

```



```{r}

## feature extracton



#install.packages("sjmisc")
library(sjmisc)

cols <- c('iid', 'gender', 'age', 'age_o', 'race_explained', 'race_explained_o', 'samerace', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr1_1', 'sinc1_1', 'intel1_1', 'amb1_1', 'shar1_1', 'attr', 'sinc', 'intel', 'amb', 'shar', 'attr_o', 'sinc_o', 'intel_o', 'amb_o', 'shar_o', 'like', 'like_o', 'prob', 'prob_o', 'match')

df <- subset(SD, select = c('iid', 'attr', 'attr3_1', 'match')) %>%
  mutate(diff = attr3_1 - attr) %>%
  group_by(iid) %>%
  summarise(mean_diff = mean(diff, na.rm = T),
            type = case_when(mean_diff < -1.5 ~ 'underestimated',
                             mean_diff > 1.5 ~ 'overestimated',
                             TRUE ~ 'realistic')) %>%
  add_column(positive_rate = positive_rate$Rate, match_rate = match_rate$match_rate)
df

age_mean <- as.integer(mean(SD$age, na.rm = T))

data <- SD[, cols]
data <- left_join(data, df, by = 'iid') %>%
  tidyr::replace_na(list(age = age_mean, age_o = age_mean)) %>%
  mutate(age_gap = abs(age - age_o)) %>%
  subset(select = c(-age, -age_o, -iid, -type, -race_explained, -race_explained_o, -positive_rate, -match_rate)) %>%
  drop_na() %>% 
  mutate(gender = as.factor(gender),
         samerace = as.factor(samerace),
         match = as.factor(match))
  
sum(is.na(data)); dim(SD); dim(data); colnames(data)

levels(data$match) # Need to change the levels because the models view the first level as positive
data <- data %>%
  mutate(match = fct_rev(match))

```

## Building a model (KNN)

```{r}
set.seed(456)
data_split <- initial_split(data, strata = match)
train <- training(data_split)
test <- testing(data_split)

set.seed(456)
folds <- vfold_cv(train, strata = match)
folds

# Defining a recipe
data_rec <- recipe(match ~ ., data = train) %>%
  step_normalize(all_numeric())

# Executing the recipe
norm <- data_rec %>% prep() %>% juice()
norm %>% select(where(is.numeric)) %>% sapply(mean)
norm %>% select(where(is.numeric)) %>% sapply(sd)

# Selecting metrics
c_metrics <- metric_set(accuracy, sens, roc_auc, mn_log_loss)

# Saving predictions
model_control <- control_grid(save_pred = T)

# Defining a model
#install.packages('kknn')
library(kknn)
knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_mode('classification') %>%
  set_engine('kknn')

knn_grid <- grid_regular(parameters(knn_spec), levels = 5)

knn_tune <- tune_grid(
  knn_spec,
  data_rec,
  resamples = folds,
  control = model_control,
  metrics = c_metrics
)

# Plotting metrics
collect_metrics(knn_tune) %>%
  ggplot(aes(x = neighbors, y = mean)) + 
  geom_point() + geom_line() + 
  facet_wrap(~.metric, scales = 'free_y')

# to see variance between each fold
knn_tune %>% select(id, .metrics) %>%
  unnest(.metrics) %>%
  ggplot(aes(x = neighbors, y = .estimate, color = id)) + geom_point() + geom_line() + 
  facet_wrap(~.metric, scales = 'free_y')

data_metrics <- knn_tune %>% collect_predictions() %>%
  mutate(pred = if_else(.pred_1 >= .5, 1, 0),
         pred = as.factor(pred),
         pred = fct_rev(pred)) 

data_metrics %>%
  group_by(id) %>%
  summarise()

data_metrics %>%
  conf_mat(match, pred) #quite a lot of false negatives :/

data_metrics %>% accuracy(match, pred)
data_metrics %>% sens(match, pred) # pooor sensitivity
data_metrics %>% ppv(match, pred) 

# roc curve
knn_tune %>% collect_predictions() %>%
  group_by(id) %>%
  roc_curve(match, .pred_1) %>%
  autoplot()

# gain curve - how much of the class I encapture given the class probabilities, that is, if we say that everyone with the 50% prob. or less being a "match" (1) class, we can capture around 80% of the actual points
knn_tune %>% 
  collect_predictions() %>%
  gain_curve(match, .pred_1) %>%
  autoplot()
  
# Collect metrics and fit to a model
knn_tune %>% select_best(metric = 'roc_auc')
knn_tune %>% select_best(metric = 'accuracy')

# Defining a final model
knn_spec_final <- nearest_neighbor(neighbors = 14) %>%
  set_mode('classification') %>%
  set_engine('kknn')

# Defining a workflow
final_model <- workflow() %>%
  add_model(knn_spec_final) %>%
  add_recipe(data_rec)

# Fitting a final model
final_results <- last_fit(final_model, data_split)

# if we say 50% of these people got match, around 50% of those people actually got match
final_results %>% collect_predictions() %>%
  select(.pred_1, match) %>%
  mutate(.pred = 100*.pred_1) %>%
  select(-.pred_1) %>%
  mutate(.pred = round(.pred/5)*5) %>% #rounding to 5
  count(.pred, match) %>%
  pivot_wider(names_from = match, values_from = n) %>%
  rename(Yes = `1`, No = `0`) %>%
  mutate(prob = Yes/(Yes + No)) %>%
  mutate(prob = prob*100) %>%
  ggplot(aes(x = .pred, y = prob)) + geom_point() + geom_smooth() + geom_abline() +
  coord_fixed(ylim = c(0,100), xlim = c(0, 100))

```















>>>>>>> b8dcfa91156edf0dad14913496e588ac2bf90a97
